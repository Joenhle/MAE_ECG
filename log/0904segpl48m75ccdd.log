cuda is True
device is cuda
freeze
training with pre_train
linearprobe,freeze encoder
encoder.cls_token
False
encoder.pos_embed
False
encoder.patch_embed.proj.weight
False
encoder.patch_embed.proj.bias
False
encoder.blocks.0.norm1.weight
False
encoder.blocks.0.norm1.bias
False
encoder.blocks.0.attn.qkv.weight
False
encoder.blocks.0.attn.qkv.bias
False
encoder.blocks.0.attn.proj.weight
False
encoder.blocks.0.attn.proj.bias
False
encoder.blocks.0.norm2.weight
False
encoder.blocks.0.norm2.bias
False
encoder.blocks.0.mlp.fc1.weight
False
encoder.blocks.0.mlp.fc1.bias
False
encoder.blocks.0.mlp.fc2.weight
False
encoder.blocks.0.mlp.fc2.bias
False
encoder.blocks.1.norm1.weight
False
encoder.blocks.1.norm1.bias
False
encoder.blocks.1.attn.qkv.weight
False
encoder.blocks.1.attn.qkv.bias
False
encoder.blocks.1.attn.proj.weight
False
encoder.blocks.1.attn.proj.bias
False
encoder.blocks.1.norm2.weight
False
encoder.blocks.1.norm2.bias
False
encoder.blocks.1.mlp.fc1.weight
False
encoder.blocks.1.mlp.fc1.bias
False
encoder.blocks.1.mlp.fc2.weight
False
encoder.blocks.1.mlp.fc2.bias
False
encoder.blocks.2.norm1.weight
False
encoder.blocks.2.norm1.bias
False
encoder.blocks.2.attn.qkv.weight
False
encoder.blocks.2.attn.qkv.bias
False
encoder.blocks.2.attn.proj.weight
False
encoder.blocks.2.attn.proj.bias
False
encoder.blocks.2.norm2.weight
False
encoder.blocks.2.norm2.bias
False
encoder.blocks.2.mlp.fc1.weight
False
encoder.blocks.2.mlp.fc1.bias
False
encoder.blocks.2.mlp.fc2.weight
False
encoder.blocks.2.mlp.fc2.bias
False
encoder.blocks.3.norm1.weight
False
encoder.blocks.3.norm1.bias
False
encoder.blocks.3.attn.qkv.weight
False
encoder.blocks.3.attn.qkv.bias
False
encoder.blocks.3.attn.proj.weight
False
encoder.blocks.3.attn.proj.bias
False
encoder.blocks.3.norm2.weight
False
encoder.blocks.3.norm2.bias
False
encoder.blocks.3.mlp.fc1.weight
False
encoder.blocks.3.mlp.fc1.bias
False
encoder.blocks.3.mlp.fc2.weight
False
encoder.blocks.3.mlp.fc2.bias
False
encoder.blocks.4.norm1.weight
False
encoder.blocks.4.norm1.bias
False
encoder.blocks.4.attn.qkv.weight
False
encoder.blocks.4.attn.qkv.bias
False
encoder.blocks.4.attn.proj.weight
False
encoder.blocks.4.attn.proj.bias
False
encoder.blocks.4.norm2.weight
False
encoder.blocks.4.norm2.bias
False
encoder.blocks.4.mlp.fc1.weight
False
encoder.blocks.4.mlp.fc1.bias
False
encoder.blocks.4.mlp.fc2.weight
False
encoder.blocks.4.mlp.fc2.bias
False
encoder.blocks.5.norm1.weight
False
encoder.blocks.5.norm1.bias
False
encoder.blocks.5.attn.qkv.weight
False
encoder.blocks.5.attn.qkv.bias
False
encoder.blocks.5.attn.proj.weight
False
encoder.blocks.5.attn.proj.bias
False
encoder.blocks.5.norm2.weight
False
encoder.blocks.5.norm2.bias
False
encoder.blocks.5.mlp.fc1.weight
False
encoder.blocks.5.mlp.fc1.bias
False
encoder.blocks.5.mlp.fc2.weight
False
encoder.blocks.5.mlp.fc2.bias
False
encoder.blocks.6.norm1.weight
False
encoder.blocks.6.norm1.bias
False
encoder.blocks.6.attn.qkv.weight
False
encoder.blocks.6.attn.qkv.bias
False
encoder.blocks.6.attn.proj.weight
False
encoder.blocks.6.attn.proj.bias
False
encoder.blocks.6.norm2.weight
False
encoder.blocks.6.norm2.bias
False
encoder.blocks.6.mlp.fc1.weight
False
encoder.blocks.6.mlp.fc1.bias
False
encoder.blocks.6.mlp.fc2.weight
False
encoder.blocks.6.mlp.fc2.bias
False
encoder.blocks.7.norm1.weight
False
encoder.blocks.7.norm1.bias
False
encoder.blocks.7.attn.qkv.weight
False
encoder.blocks.7.attn.qkv.bias
False
encoder.blocks.7.attn.proj.weight
False
encoder.blocks.7.attn.proj.bias
False
encoder.blocks.7.norm2.weight
False
encoder.blocks.7.norm2.bias
False
encoder.blocks.7.mlp.fc1.weight
False
encoder.blocks.7.mlp.fc1.bias
False
encoder.blocks.7.mlp.fc2.weight
False
encoder.blocks.7.mlp.fc2.bias
False
encoder.blocks.8.norm1.weight
False
encoder.blocks.8.norm1.bias
False
encoder.blocks.8.attn.qkv.weight
False
encoder.blocks.8.attn.qkv.bias
False
encoder.blocks.8.attn.proj.weight
False
encoder.blocks.8.attn.proj.bias
False
encoder.blocks.8.norm2.weight
False
encoder.blocks.8.norm2.bias
False
encoder.blocks.8.mlp.fc1.weight
False
encoder.blocks.8.mlp.fc1.bias
False
encoder.blocks.8.mlp.fc2.weight
False
encoder.blocks.8.mlp.fc2.bias
False
encoder.blocks.9.norm1.weight
False
encoder.blocks.9.norm1.bias
False
encoder.blocks.9.attn.qkv.weight
False
encoder.blocks.9.attn.qkv.bias
False
encoder.blocks.9.attn.proj.weight
False
encoder.blocks.9.attn.proj.bias
False
encoder.blocks.9.norm2.weight
False
encoder.blocks.9.norm2.bias
False
encoder.blocks.9.mlp.fc1.weight
False
encoder.blocks.9.mlp.fc1.bias
False
encoder.blocks.9.mlp.fc2.weight
False
encoder.blocks.9.mlp.fc2.bias
False
encoder.blocks.10.norm1.weight
False
encoder.blocks.10.norm1.bias
False
encoder.blocks.10.attn.qkv.weight
False
encoder.blocks.10.attn.qkv.bias
False
encoder.blocks.10.attn.proj.weight
False
encoder.blocks.10.attn.proj.bias
False
encoder.blocks.10.norm2.weight
False
encoder.blocks.10.norm2.bias
False
encoder.blocks.10.mlp.fc1.weight
False
encoder.blocks.10.mlp.fc1.bias
False
encoder.blocks.10.mlp.fc2.weight
False
encoder.blocks.10.mlp.fc2.bias
False
encoder.blocks.11.norm1.weight
False
encoder.blocks.11.norm1.bias
False
encoder.blocks.11.attn.qkv.weight
False
encoder.blocks.11.attn.qkv.bias
False
encoder.blocks.11.attn.proj.weight
False
encoder.blocks.11.attn.proj.bias
False
encoder.blocks.11.norm2.weight
False
encoder.blocks.11.norm2.bias
False
encoder.blocks.11.mlp.fc1.weight
False
encoder.blocks.11.mlp.fc1.bias
False
encoder.blocks.11.mlp.fc2.weight
False
encoder.blocks.11.mlp.fc2.bias
False
encoder.norm.weight
False
encoder.norm.bias
False
upsample_1_1.weight
True
upsample_1_1.bias
True
upsample_2_1.weight
True
upsample_2_1.bias
True
upsample_3_1.weight
True
upsample_3_1.bias
True
upsample_4_1.weight
True
upsample_4_1.bias
True
upsample_1_2.weight
True
upsample_1_2.bias
True
upsample_2_2.weight
True
upsample_2_2.bias
True
upsample_3_2.weight
True
upsample_3_2.bias
True
upsample_4_2.weight
True
upsample_4_2.bias
True
conv_out.seq.0.weight
True
conv_out.seq.1.weight
True
conv_out.seq.1.bias
True
conv_cat1.seq.0.weight
True
conv_cat1.seq.1.weight
True
conv_cat1.seq.1.bias
True
conv_cat2.seq.0.weight
True
conv_cat2.seq.1.weight
True
conv_cat2.seq.1.bias
True
conv_cat3.seq.0.weight
True
conv_cat3.seq.1.weight
True
conv_cat3.seq.1.bias
True
conv_cat4.seq.0.weight
True
conv_cat4.seq.1.weight
True
conv_cat4.seq.1.bias
True
ECG_mae_segmentation_U_48(
  (encoder): EncoderMAE(
    (patch_embed): PatchEmbed_1D(
      (proj): Conv1d(1, 160, kernel_size=(48,), stride=(48,))
      (norm): Identity()
    )
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=160, out_features=480, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=160, out_features=160, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=160, out_features=320, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=320, out_features=160, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
  )
  (upsample_1_1): ConvTranspose1d(160, 80, kernel_size=(10,), stride=(4,), padding=(3,))
  (upsample_2_1): ConvTranspose1d(80, 40, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_3_1): ConvTranspose1d(40, 10, kernel_size=(9,), stride=(3,), padding=(3,))
  (upsample_4_1): ConvTranspose1d(10, 4, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_1_2): ConvTranspose1d(160, 80, kernel_size=(10,), stride=(4,), padding=(3,))
  (upsample_2_2): ConvTranspose1d(80, 40, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_3_2): ConvTranspose1d(40, 10, kernel_size=(9,), stride=(3,), padding=(3,))
  (upsample_4_2): ConvTranspose1d(10, 4, kernel_size=(8,), stride=(2,), padding=(3,))
  (conv_out): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(4, 4, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat1): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(160, 80, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat2): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(80, 40, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat3): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(20, 10, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat4): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(8, 4, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
)  0%|          | 0/1000 [00:00<?, ?it/s]
train_datasize 702 val_datasize 159
  0%|          | 1/1000 [00:40<11:20:05, 40.85s/it]#epoch:01 stage:1 train_loss:1.108e-02 val_loss:9.904e-03  time:0m41s

 bg_pi:0.181 bg_ri:0.895 p_pi:0.565 p_ri:0.906 r_pi:0.620 r_ri:0.718 t_pi:0.918 t_ri:0.205
  0%|          | 2/1000 [01:23<11:26:04, 41.25s/it]#epoch:02 stage:1 train_loss:1.030e-02 val_loss:9.129e-03  time:0m42s

 bg_pi:0.285 bg_ri:0.891 p_pi:0.480 p_ri:0.962 r_pi:0.590 r_ri:0.922 t_pi:0.949 t_ri:0.303
  0%|          | 3/1000 [02:07<11:40:47, 42.17s/it]#epoch:03 stage:1 train_loss:9.734e-03 val_loss:8.526e-03  time:0m44s

 bg_pi:0.380 bg_ri:0.891 p_pi:0.477 p_ri:0.964 r_pi:0.552 r_ri:0.977 t_pi:0.975 t_ri:0.371
ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
 ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm).
   0%|          | 3/1000 [02:07<11:47:14, 42.56s/it]
Traceback (most recent call last):
  File "train.py", line 260, in <module>
    train(args)
  File "train.py", line 81, in train
    train_procedure(args,model,model_save_dir)
  File "train.py", line 112, in train_procedure
    train_loss = train_epoch(model, optimizer, criterion,train_dataloader, show_interval=100)/len_train_dataset
  File "train.py", line 150, in train_epoch
    for inputs,target in train_dataloader:
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 435, in __next__
    data = self._next_data()
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1085, in _next_data
    return self._process_data(data)
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 1111, in _process_data
    data.reraise()
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/_utils.py", line 428, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in DataLoader worker process 1.
Original Traceback (most recent call last):
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py", line 198, in _worker_loop
    data = fetcher.fetch(index)
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py", line 47, in fetch
    return self.collate_fn(data)
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py", line 83, in default_collate
    return [default_collate(samples) for samples in transposed]
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py", line 83, in <listcomp>
    return [default_collate(samples) for samples in transposed]
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/utils/data/_utils/collate.py", line 53, in default_collate
    storage = elem.storage()._new_shared(numel)
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/storage.py", line 137, in _new_shared
    return cls._new_using_fd(size)
RuntimeError: unable to write to file </torch_8888_3951619863>

Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/root/miniconda3/envs/ECG/lib/python3.7/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 8889) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.

start train
ECG_mae_segmentation(
  (mae): MAE_linearmask(
    (encoder): ViT1D(
      (to_patch_embedding): Sequential(
        (0): Rearrange('b (sl pl) -> b sl pl', pl=10)
        (1): Linear(in_features=10, out_features=8, bias=True)
      )
      (dropout): Dropout(p=0.0, inplace=False)
      (transformer): Transformer(
        (layers): ModuleList(
          (0): ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=8, out_features=96, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=32, out_features=8, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=8, out_features=8, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=8, out_features=8, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (1): ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=8, out_features=96, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=32, out_features=8, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=8, out_features=8, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=8, out_features=8, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (2): ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=8, out_features=96, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=32, out_features=8, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=8, out_features=8, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=8, out_features=8, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
          (3): ModuleList(
            (0): PreNorm(
              (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
              (fn): Attention(
                (attend): Softmax(dim=-1)
                (to_qkv): Linear(in_features=8, out_features=96, bias=False)
                (to_out): Sequential(
                  (0): Linear(in_features=32, out_features=8, bias=True)
                  (1): Dropout(p=0.0, inplace=False)
                )
              )
            )
            (1): PreNorm(
              (norm): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
              (fn): FeedForward(
                (net): Sequential(
                  (0): Linear(in_features=8, out_features=8, bias=True)
                  (1): GELU()
                  (2): Dropout(p=0.0, inplace=False)
                  (3): Linear(in_features=8, out_features=8, bias=True)
                  (4): Dropout(p=0.0, inplace=False)
                )
              )
            )
          )
        )
      )
      (to_latent): Identity()
      (mlp_head): Sequential(
        (0): LayerNorm((8,), eps=1e-05, elementwise_affine=True)
        (1): Linear(in_features=8, out_features=1000, bias=True)
      )
    )
    (to_patch): Rearrange('b (sl pl) -> b sl pl', pl=10)
    (patch_to_emb): Linear(in_features=10, out_features=8, bias=True)
    (to_linear_mask): Linear_mask()
    (enc_to_dec): Linear(in_features=8, out_features=5, bias=True)
    (decoder): Transformer(
      (layers): ModuleList(
        (0): ModuleList(
          (0): PreNorm(
            (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (attend): Softmax(dim=-1)
              (to_qkv): Linear(in_features=5, out_features=192, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=5, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): PreNorm(
            (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=5, out_features=20, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=20, out_features=5, bias=True)
                (4): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (1): ModuleList(
          (0): PreNorm(
            (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (attend): Softmax(dim=-1)
              (to_qkv): Linear(in_features=5, out_features=192, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=5, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): PreNorm(
            (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=5, out_features=20, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=20, out_features=5, bias=True)
                (4): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (2): ModuleList(
          (0): PreNorm(
            (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (attend): Softmax(dim=-1)
              (to_qkv): Linear(in_features=5, out_features=192, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=5, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): PreNorm(
            (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=5, out_features=20, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=20, out_features=5, bias=True)
                (4): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
        (3): ModuleList(
          (0): PreNorm(
            (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
            (fn): Attention(
              (attend): Softmax(dim=-1)
              (to_qkv): Linear(in_features=5, out_features=192, bias=False)
              (to_out): Sequential(
                (0): Linear(in_features=64, out_features=5, bias=True)
                (1): Dropout(p=0.0, inplace=False)
              )
            )
          )
          (1): PreNorm(
            (norm): LayerNorm((5,), eps=1e-05, elementwise_affine=True)
            (fn): FeedForward(
              (net): Sequential(
                (0): Linear(in_features=5, out_features=20, bias=True)
                (1): GELU()
                (2): Dropout(p=0.0, inplace=False)
                (3): Linear(in_features=20, out_features=5, bias=True)
                (4): Dropout(p=0.0, inplace=False)
              )
            )
          )
        )
      )
    )
    (decoder_pos_emb): Embedding(251, 5)
    (to_pixels): Linear(in_features=5, out_features=10, bias=True)
    (loss_funtion): MSELoss()
  )
  (upsample_1): ConvTranspose1d(5, 4, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_2): ConvTranspose1d(4, 4, kernel_size=(8,), stride=(5,), padding=(3,), output_padding=(3,))
  (conv_out): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(4, 4, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
)/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))

  0%|          | 0/200 [00:00<?, ?it/s]
train_datasize 1662 val_datasize 399

  0%|          | 1/200 [01:18<4:21:53, 78.96s/it]#epoch:01 stage:1 train_loss:2.369e-02 val_loss:2.164e-02  time:1m19s


  1%|          | 2/200 [02:34<4:17:18, 77.97s/it]#epoch:02 stage:1 train_loss:2.143e-02 val_loss:2.042e-02  time:1m16s


  2%|▏         | 3/200 [03:52<4:16:06, 78.00s/it]#epoch:03 stage:1 train_loss:2.044e-02 val_loss:1.997e-02  time:1m18s


  2%|▏         | 4/200 [05:09<4:13:08, 77.49s/it]#epoch:04 stage:1 train_loss:2.006e-02 val_loss:1.976e-02  time:1m16s


  2%|▎         | 5/200 [06:25<4:10:58, 77.22s/it]#epoch:05 stage:1 train_loss:1.989e-02 val_loss:1.957e-02  time:1m17s


  3%|▎         | 6/200 [07:42<4:09:40, 77.22s/it]#epoch:06 stage:1 train_loss:1.980e-02 val_loss:1.944e-02  time:1m17s


  4%|▎         | 7/200 [09:00<4:08:47, 77.35s/it]#epoch:07 stage:1 train_loss:1.972e-02 val_loss:1.944e-02  time:1m18s


  4%|▍         | 8/200 [09:55<3:45:59, 70.62s/it]#epoch:08 stage:1 train_loss:1.966e-02 val_loss:1.937e-02  time:0m55s


  4%|▍         | 9/200 [10:29<3:09:30, 59.53s/it]#epoch:09 stage:1 train_loss:1.960e-02 val_loss:1.924e-02  time:0m34s


  5%|▌         | 10/200 [11:02<2:43:51, 51.75s/it]#epoch:10 stage:1 train_loss:1.955e-02 val_loss:1.927e-02  time:0m34s


  6%|▌         | 11/200 [11:36<2:25:40, 46.25s/it]#epoch:11 stage:1 train_loss:1.951e-02 val_loss:1.921e-02  time:0m33s


  6%|▌         | 12/200 [12:09<2:13:20, 42.56s/it]#epoch:12 stage:1 train_loss:1.947e-02 val_loss:1.915e-02  time:0m34s


  6%|▋         | 13/200 [12:43<2:04:33, 39.96s/it]#epoch:13 stage:1 train_loss:1.943e-02 val_loss:1.911e-02  time:0m34s


  7%|▋         | 14/200 [13:18<1:58:32, 38.24s/it]#epoch:14 stage:1 train_loss:1.940e-02 val_loss:1.915e-02  time:0m34s


  8%|▊         | 15/200 [13:52<1:54:04, 36.99s/it]#epoch:15 stage:1 train_loss:1.938e-02 val_loss:1.912e-02  time:0m34s


  8%|▊         | 16/200 [14:25<1:50:30, 36.04s/it]#epoch:16 stage:1 train_loss:1.935e-02 val_loss:1.905e-02  time:0m34s


  8%|▊         | 17/200 [14:59<1:47:46, 35.34s/it]#epoch:17 stage:1 train_loss:1.934e-02 val_loss:1.913e-02  time:0m34s


  9%|▉         | 18/200 [15:31<1:44:03, 34.31s/it]#epoch:18 stage:1 train_loss:1.931e-02 val_loss:1.905e-02  time:0m32s


 10%|▉         | 19/200 [16:06<1:43:43, 34.38s/it]#epoch:19 stage:1 train_loss:1.928e-02 val_loss:1.899e-02  time:0m35s


 10%|█         | 20/200 [16:41<1:43:43, 34.58s/it]#epoch:20 stage:1 train_loss:1.927e-02 val_loss:1.901e-02  time:0m35s


 10%|█         | 21/200 [17:15<1:42:38, 34.41s/it]#epoch:21 stage:1 train_loss:1.923e-02 val_loss:1.898e-02  time:0m34s


 11%|█         | 22/200 [17:50<1:42:57, 34.70s/it]#epoch:22 stage:1 train_loss:1.922e-02 val_loss:1.893e-02  time:0m35s


 12%|█▏        | 23/200 [18:24<1:41:55, 34.55s/it]#epoch:23 stage:1 train_loss:1.921e-02 val_loss:1.895e-02  time:0m34s


 12%|█▏        | 24/200 [18:58<1:40:46, 34.36s/it]#epoch:24 stage:1 train_loss:1.918e-02 val_loss:1.889e-02  time:0m34s


 12%|█▎        | 25/200 [19:34<1:41:21, 34.75s/it]#epoch:25 stage:1 train_loss:1.918e-02 val_loss:1.891e-02  time:0m36s


 13%|█▎        | 26/200 [20:08<1:40:11, 34.55s/it]#epoch:26 stage:1 train_loss:1.916e-02 val_loss:1.888e-02  time:0m34s


 14%|█▎        | 27/200 [20:44<1:40:46, 34.95s/it]#epoch:27 stage:1 train_loss:1.915e-02 val_loss:1.883e-02  time:0m36s


 14%|█▍        | 28/200 [21:19<1:40:40, 35.12s/it]#epoch:28 stage:1 train_loss:1.912e-02 val_loss:1.880e-02  time:0m36s


 14%|█▍        | 29/200 [21:54<1:39:32, 34.93s/it]#epoch:29 stage:1 train_loss:1.911e-02 val_loss:1.882e-02  time:0m34s


 15%|█▌        | 30/200 [22:28<1:38:08, 34.64s/it]#epoch:30 stage:1 train_loss:1.909e-02 val_loss:1.879e-02  time:0m34s


 16%|█▌        | 31/200 [23:02<1:37:07, 34.48s/it]#epoch:31 stage:1 train_loss:1.907e-02 val_loss:1.881e-02  time:0m34s


 16%|█▌        | 32/200 [23:36<1:36:35, 34.50s/it]#epoch:32 stage:1 train_loss:1.906e-02 val_loss:1.882e-02  time:0m35s


 16%|█▋        | 33/200 [24:11<1:35:59, 34.49s/it]#epoch:33 stage:1 train_loss:1.904e-02 val_loss:1.878e-02  time:0m34s


 17%|█▋        | 34/200 [24:45<1:35:30, 34.52s/it]#epoch:34 stage:1 train_loss:1.905e-02 val_loss:1.874e-02  time:0m35s


 18%|█▊        | 35/200 [25:20<1:34:40, 34.43s/it]#epoch:35 stage:1 train_loss:1.904e-02 val_loss:1.870e-02  time:0m34s


 18%|█▊        | 36/200 [25:55<1:34:38, 34.62s/it]#epoch:36 stage:1 train_loss:1.901e-02 val_loss:1.871e-02  time:0m35s


 18%|█▊        | 37/200 [26:29<1:33:41, 34.49s/it]#epoch:37 stage:1 train_loss:1.899e-02 val_loss:1.868e-02  time:0m34s


 19%|█▉        | 38/200 [27:04<1:33:21, 34.58s/it]#epoch:38 stage:1 train_loss:1.898e-02 val_loss:1.868e-02  time:0m35s


 19%|█▉        | 38/200 [27:22<1:56:41, 43.22s/it]
Traceback (most recent call last):
  File "train.py", line 142, in <module>
    out_pred = np.array([output_sliding_voting(i,9) for i in out_pred])
  File "train.py", line 51, in train
    model = model.to(device)
  File "train.py", line 78, in train_procedure
    summary_writer = SummaryWriter(log_dir=model_save_dir,flush_secs=2)
  File "train.py", line 108, in train_epoch
    model.train()
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ECG_AI/MAE_ECG/models/ECG_mae_segmentation.py", line 37, in forward
    decoder_tokens = self.mae(x)
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ECG_AI/MAE_ECG/models/mae_1D_linearmask.py", line 201, in forward
    patches = self.to_linear_mask(patches,masked_indices)
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/ECG_AI/MAE_ECG/models/mae_1D_linearmask.py", line 135, in forward
    new_patches[b,index] = new_patch
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 159686) is killed by signal: Terminated. 

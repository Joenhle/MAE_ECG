cuda is True
device is cuda
freeze
training with pre_train
linearprobe,freeze encoder
encoder.cls_token
False
encoder.pos_embed
False
encoder.patch_embed.proj.weight
False
encoder.patch_embed.proj.bias
False
encoder.blocks.0.norm1.weight
False
encoder.blocks.0.norm1.bias
False
encoder.blocks.0.attn.qkv.weight
False
encoder.blocks.0.attn.qkv.bias
False
encoder.blocks.0.attn.proj.weight
False
encoder.blocks.0.attn.proj.bias
False
encoder.blocks.0.norm2.weight
False
encoder.blocks.0.norm2.bias
False
encoder.blocks.0.mlp.fc1.weight
False
encoder.blocks.0.mlp.fc1.bias
False
encoder.blocks.0.mlp.fc2.weight
False
encoder.blocks.0.mlp.fc2.bias
False
encoder.blocks.1.norm1.weight
False
encoder.blocks.1.norm1.bias
False
encoder.blocks.1.attn.qkv.weight
False
encoder.blocks.1.attn.qkv.bias
False
encoder.blocks.1.attn.proj.weight
False
encoder.blocks.1.attn.proj.bias
False
encoder.blocks.1.norm2.weight
False
encoder.blocks.1.norm2.bias
False
encoder.blocks.1.mlp.fc1.weight
False
encoder.blocks.1.mlp.fc1.bias
False
encoder.blocks.1.mlp.fc2.weight
False
encoder.blocks.1.mlp.fc2.bias
False
encoder.blocks.2.norm1.weight
False
encoder.blocks.2.norm1.bias
False
encoder.blocks.2.attn.qkv.weight
False
encoder.blocks.2.attn.qkv.bias
False
encoder.blocks.2.attn.proj.weight
False
encoder.blocks.2.attn.proj.bias
False
encoder.blocks.2.norm2.weight
False
encoder.blocks.2.norm2.bias
False
encoder.blocks.2.mlp.fc1.weight
False
encoder.blocks.2.mlp.fc1.bias
False
encoder.blocks.2.mlp.fc2.weight
False
encoder.blocks.2.mlp.fc2.bias
False
encoder.blocks.3.norm1.weight
False
encoder.blocks.3.norm1.bias
False
encoder.blocks.3.attn.qkv.weight
False
encoder.blocks.3.attn.qkv.bias
False
encoder.blocks.3.attn.proj.weight
False
encoder.blocks.3.attn.proj.bias
False
encoder.blocks.3.norm2.weight
False
encoder.blocks.3.norm2.bias
False
encoder.blocks.3.mlp.fc1.weight
False
encoder.blocks.3.mlp.fc1.bias
False
encoder.blocks.3.mlp.fc2.weight
False
encoder.blocks.3.mlp.fc2.bias
False
encoder.blocks.4.norm1.weight
False
encoder.blocks.4.norm1.bias
False
encoder.blocks.4.attn.qkv.weight
False
encoder.blocks.4.attn.qkv.bias
False
encoder.blocks.4.attn.proj.weight
False
encoder.blocks.4.attn.proj.bias
False
encoder.blocks.4.norm2.weight
False
encoder.blocks.4.norm2.bias
False
encoder.blocks.4.mlp.fc1.weight
False
encoder.blocks.4.mlp.fc1.bias
False
encoder.blocks.4.mlp.fc2.weight
False
encoder.blocks.4.mlp.fc2.bias
False
encoder.blocks.5.norm1.weight
False
encoder.blocks.5.norm1.bias
False
encoder.blocks.5.attn.qkv.weight
False
encoder.blocks.5.attn.qkv.bias
False
encoder.blocks.5.attn.proj.weight
False
encoder.blocks.5.attn.proj.bias
False
encoder.blocks.5.norm2.weight
False
encoder.blocks.5.norm2.bias
False
encoder.blocks.5.mlp.fc1.weight
False
encoder.blocks.5.mlp.fc1.bias
False
encoder.blocks.5.mlp.fc2.weight
False
encoder.blocks.5.mlp.fc2.bias
False
encoder.blocks.6.norm1.weight
False
encoder.blocks.6.norm1.bias
False
encoder.blocks.6.attn.qkv.weight
False
encoder.blocks.6.attn.qkv.bias
False
encoder.blocks.6.attn.proj.weight
False
encoder.blocks.6.attn.proj.bias
False
encoder.blocks.6.norm2.weight
False
encoder.blocks.6.norm2.bias
False
encoder.blocks.6.mlp.fc1.weight
False
encoder.blocks.6.mlp.fc1.bias
False
encoder.blocks.6.mlp.fc2.weight
False
encoder.blocks.6.mlp.fc2.bias
False
encoder.blocks.7.norm1.weight
False
encoder.blocks.7.norm1.bias
False
encoder.blocks.7.attn.qkv.weight
False
encoder.blocks.7.attn.qkv.bias
False
encoder.blocks.7.attn.proj.weight
False
encoder.blocks.7.attn.proj.bias
False
encoder.blocks.7.norm2.weight
False
encoder.blocks.7.norm2.bias
False
encoder.blocks.7.mlp.fc1.weight
False
encoder.blocks.7.mlp.fc1.bias
False
encoder.blocks.7.mlp.fc2.weight
False
encoder.blocks.7.mlp.fc2.bias
False
encoder.blocks.8.norm1.weight
False
encoder.blocks.8.norm1.bias
False
encoder.blocks.8.attn.qkv.weight
False
encoder.blocks.8.attn.qkv.bias
False
encoder.blocks.8.attn.proj.weight
False
encoder.blocks.8.attn.proj.bias
False
encoder.blocks.8.norm2.weight
False
encoder.blocks.8.norm2.bias
False
encoder.blocks.8.mlp.fc1.weight
False
encoder.blocks.8.mlp.fc1.bias
False
encoder.blocks.8.mlp.fc2.weight
False
encoder.blocks.8.mlp.fc2.bias
False
encoder.blocks.9.norm1.weight
False
encoder.blocks.9.norm1.bias
False
encoder.blocks.9.attn.qkv.weight
False
encoder.blocks.9.attn.qkv.bias
False
encoder.blocks.9.attn.proj.weight
False
encoder.blocks.9.attn.proj.bias
False
encoder.blocks.9.norm2.weight
False
encoder.blocks.9.norm2.bias
False
encoder.blocks.9.mlp.fc1.weight
False
encoder.blocks.9.mlp.fc1.bias
False
encoder.blocks.9.mlp.fc2.weight
False
encoder.blocks.9.mlp.fc2.bias
False
encoder.blocks.10.norm1.weight
False
encoder.blocks.10.norm1.bias
False
encoder.blocks.10.attn.qkv.weight
False
encoder.blocks.10.attn.qkv.bias
False
encoder.blocks.10.attn.proj.weight
False
encoder.blocks.10.attn.proj.bias
False
encoder.blocks.10.norm2.weight
False
encoder.blocks.10.norm2.bias
False
encoder.blocks.10.mlp.fc1.weight
False
encoder.blocks.10.mlp.fc1.bias
False
encoder.blocks.10.mlp.fc2.weight
False
encoder.blocks.10.mlp.fc2.bias
False
encoder.blocks.11.norm1.weight
False
encoder.blocks.11.norm1.bias
False
encoder.blocks.11.attn.qkv.weight
False
encoder.blocks.11.attn.qkv.bias
False
encoder.blocks.11.attn.proj.weight
False
encoder.blocks.11.attn.proj.bias
False
encoder.blocks.11.norm2.weight
False
encoder.blocks.11.norm2.bias
False
encoder.blocks.11.mlp.fc1.weight
False
encoder.blocks.11.mlp.fc1.bias
False
encoder.blocks.11.mlp.fc2.weight
False
encoder.blocks.11.mlp.fc2.bias
False
encoder.norm.weight
False
encoder.norm.bias
False
upsample_1_1.weight
True
upsample_1_1.bias
True
upsample_2_1.weight
True
upsample_2_1.bias
True
upsample_3_1.weight
True
upsample_3_1.bias
True
upsample_4_1.weight
True
upsample_4_1.bias
True
upsample_1_2.weight
True
upsample_1_2.bias
True
upsample_2_2.weight
True
upsample_2_2.bias
True
upsample_3_2.weight
True
upsample_3_2.bias
True
upsample_4_2.weight
True
upsample_4_2.bias
True
conv_out.seq.0.weight
True
conv_out.seq.1.weight
True
conv_out.seq.1.bias
True
conv_cat1.seq.0.weight
True
conv_cat1.seq.1.weight
True
conv_cat1.seq.1.bias
True
conv_cat2.seq.0.weight
True
conv_cat2.seq.1.weight
True
conv_cat2.seq.1.bias
True
conv_cat3.seq.0.weight
True
conv_cat3.seq.1.weight
True
conv_cat3.seq.1.bias
True
conv_cat4.seq.0.weight
True
conv_cat4.seq.1.weight
True
conv_cat4.seq.1.bias
True
ECG_mae_segmentation_U_24(
  (encoder): EncoderMAE(
    (patch_embed): PatchEmbed_1D(
      (proj): Conv1d(1, 80, kernel_size=(24,), stride=(24,))
      (norm): Identity()
    )
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
  )
  (upsample_1_1): ConvTranspose1d(80, 40, kernel_size=(9,), stride=(3,), padding=(3,))
  (upsample_2_1): ConvTranspose1d(40, 20, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_3_1): ConvTranspose1d(20, 10, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_4_1): ConvTranspose1d(10, 4, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_1_2): ConvTranspose1d(80, 40, kernel_size=(9,), stride=(3,), padding=(3,))
  (upsample_2_2): ConvTranspose1d(40, 20, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_3_2): ConvTranspose1d(20, 10, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_4_2): ConvTranspose1d(10, 4, kernel_size=(8,), stride=(2,), padding=(3,))
  (conv_out): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(4, 4, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat1): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(80, 40, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat2): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(40, 20, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat3): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(20, 10, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat4): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(8, 4, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
)  0%|          | 0/1000 [00:00<?, ?it/s]
train_datasize 702 val_datasize 159
  0%|          | 1/1000 [00:41<11:38:36, 41.96s/it]#epoch:01 stage:1 train_loss:1.082e-02 val_loss:9.832e-03  time:0m42s

 bg_pi:0.273 bg_ri:0.934 p_pi:0.540 p_ri:0.910 r_pi:0.335 r_ri:0.818 t_pi:0.984 t_ri:0.076
  0%|          | 2/1000 [01:23<11:33:35, 41.70s/it]#epoch:02 stage:1 train_loss:9.947e-03 val_loss:9.058e-03  time:0m41s

 bg_pi:0.263 bg_ri:0.938 p_pi:0.450 p_ri:0.939 r_pi:0.457 r_ri:0.777 t_pi:0.992 t_ri:0.195
  0%|          | 3/1000 [02:04<11:30:44, 41.57s/it]#epoch:03 stage:1 train_loss:9.614e-03 val_loss:8.807e-03  time:0m41s

 bg_pi:0.310 bg_ri:0.936 p_pi:0.499 p_ri:0.945 r_pi:0.508 r_ri:0.822 t_pi:0.987 t_ri:0.337
  0%|          | 3/1000 [02:39<14:43:36, 53.18s/it]
Traceback (most recent call last):
  File "train.py", line 260, in <module>
    train(args)
  File "train.py", line 81, in train
    train_procedure(args,model,model_save_dir)
  File "train.py", line 114, in train_procedure
    val_loss,all_pi,all_ri= val_epoch(model, criterion, val_dataloader)
  File "train.py", line 181, in val_epoch
    y_pred = np.array([output_sliding_voting(i,9) for i in out_pred])
  File "train.py", line 181, in <listcomp>
    y_pred = np.array([output_sliding_voting(i,9) for i in out_pred])
  File "train.py", line 25, in output_sliding_voting
    output = pd.Series(output).rolling(window).apply(lambda x : mode(x)[0][0]).fillna(method='bfill')
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/pandas/core/window/rolling.py", line 2065, in apply
    kwargs=kwargs,
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/pandas/core/window/rolling.py", line 1397, in apply
    kwargs=kwargs,
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/pandas/core/window/rolling.py", line 588, in _apply
    result = calc(values)
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/pandas/core/window/rolling.py", line 576, in calc
    return func(x, start, end, min_periods)
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/pandas/core/window/rolling.py", line 1415, in apply_func
    return window_func(values, begin, end, min_periods)
  File "pandas/_libs/window/aggregations.pyx", line 1369, in pandas._libs.window.aggregations.roll_generic_fixed
  File "train.py", line 25, in <lambda>
    output = pd.Series(output).rolling(window).apply(lambda x : mode(x)[0][0]).fillna(method='bfill')
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/scipy/stats/stats.py", line 535, in mode
    contains_nan, nan_policy = _contains_nan(a, nan_policy)
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/scipy/stats/stats.py", line 224, in _contains_nan
    contains_nan = np.isnan(np.sum(a))
  File "/root/miniconda3/envs/ECG/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 267400) is killed by signal: Terminated. 

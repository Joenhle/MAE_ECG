cuda is True
device is cuda
training with pre_train
ECG_mae_segmentation_U_12(
  (encoder): EncoderMAE(
    (patch_embed): PatchEmbed_1D(
      (proj): Conv1d(1, 40, kernel_size=(12,), stride=(12,))
      (norm): Identity()
    )
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (1): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (2): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (3): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (4): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (5): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (6): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (7): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (8): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (9): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (10): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
      (11): Block(
        (norm1): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=40, out_features=120, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=40, out_features=40, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (ls1): Identity()
        (drop_path1): Identity()
        (norm2): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=40, out_features=80, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=80, out_features=40, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
        (ls2): Identity()
        (drop_path2): Identity()
      )
    )
    (norm): LayerNorm((40,), eps=1e-05, elementwise_affine=True)
  )
  (upsample_1_1): ConvTranspose1d(40, 20, kernel_size=(9,), stride=(3,), padding=(3,))
  (upsample_2_1): ConvTranspose1d(20, 10, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_3_1): ConvTranspose1d(10, 6, kernel_size=(7,), stride=(1,), padding=(3,))
  (upsample_4_1): ConvTranspose1d(6, 4, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_1_2): ConvTranspose1d(40, 20, kernel_size=(9,), stride=(3,), padding=(3,))
  (upsample_2_2): ConvTranspose1d(20, 10, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_3_2): ConvTranspose1d(10, 6, kernel_size=(7,), stride=(1,), padding=(3,))
  (upsample_4_2): ConvTranspose1d(6, 4, kernel_size=(8,), stride=(2,), padding=(3,))
  (conv_out): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(4, 4, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat1): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(40, 20, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat2): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(20, 10, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat3): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(12, 6, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat4): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(8, 4, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
)
train_datasize 1662 val_datasize 399
  0%|          | 0/250 [00:00<?, ?it/s]  0%|          | 1/250 [03:09<13:05:42, 189.33s/it]#epoch:01 stage:1 train_loss:1.126e-02 val_loss:1.112e-02  time:3m9s

 bg_pi:0.367 bg_ri:0.935 p_pi:0.684 p_ri:0.886 r_pi:0.642 r_ri:0.561 t_pi:0.937 t_ri:0.602
  1%|          | 2/250 [06:14<12:52:27, 186.89s/it]#epoch:02 stage:1 train_loss:9.868e-03 val_loss:9.619e-03  time:3m5s

 bg_pi:0.394 bg_ri:0.907 p_pi:0.658 p_ri:0.953 r_pi:0.675 r_ri:0.520 t_pi:0.939 t_ri:0.658
  1%|          | 3/250 [09:23<12:53:50, 187.98s/it]#epoch:03 stage:1 train_loss:9.519e-03 val_loss:9.491e-03  time:3m9s

 bg_pi:0.385 bg_ri:0.884 p_pi:0.699 p_ri:0.952 r_pi:0.700 r_ri:0.502 t_pi:0.939 t_ri:0.697
  2%|▏         | 4/250 [12:33<12:53:03, 188.55s/it]#epoch:04 stage:1 train_loss:9.416e-03 val_loss:9.384e-03  time:3m9s

 bg_pi:0.407 bg_ri:0.887 p_pi:0.719 p_ri:0.918 r_pi:0.765 r_ri:0.500 t_pi:0.919 t_ri:0.744
  2%|▏         | 5/250 [15:42<12:51:34, 188.96s/it]#epoch:05 stage:1 train_loss:9.346e-03 val_loss:9.343e-03  time:3m10s

 bg_pi:0.426 bg_ri:0.851 p_pi:0.723 p_ri:0.913 r_pi:0.776 r_ri:0.564 t_pi:0.911 t_ri:0.753
  2%|▏         | 6/250 [18:53<12:51:04, 189.61s/it]#epoch:06 stage:1 train_loss:9.172e-03 val_loss:9.113e-03  time:3m11s

 bg_pi:0.657 bg_ri:0.879 p_pi:0.708 p_ri:0.923 r_pi:0.752 r_ri:0.935 t_pi:0.931 t_ri:0.715
  3%|▎         | 7/250 [22:02<12:46:58, 189.38s/it]#epoch:07 stage:1 train_loss:8.928e-03 val_loss:8.927e-03  time:3m9s

 bg_pi:0.684 bg_ri:0.861 p_pi:0.727 p_ri:0.893 r_pi:0.740 r_ri:0.928 t_pi:0.917 t_ri:0.733
  3%|▎         | 8/250 [25:13<12:45:17, 189.74s/it]#epoch:08 stage:1 train_loss:8.859e-03 val_loss:8.856e-03  time:3m10s

 bg_pi:0.696 bg_ri:0.864 p_pi:0.727 p_ri:0.878 r_pi:0.778 r_ri:0.922 t_pi:0.912 t_ri:0.758
  4%|▎         | 9/250 [28:23<12:42:32, 189.85s/it]#epoch:09 stage:1 train_loss:8.811e-03 val_loss:8.795e-03  time:3m10s

 bg_pi:0.710 bg_ri:0.824 p_pi:0.731 p_ri:0.849 r_pi:0.773 r_ri:0.917 t_pi:0.895 t_ri:0.769
  4%|▍         | 10/250 [31:29<12:34:23, 188.60s/it]#epoch:10 stage:1 train_loss:8.759e-03 val_loss:8.785e-03  time:3m6s

 bg_pi:0.701 bg_ri:0.848 p_pi:0.728 p_ri:0.870 r_pi:0.757 r_ri:0.926 t_pi:0.907 t_ri:0.751
  4%|▍         | 11/250 [34:36<12:29:56, 188.27s/it]#epoch:11 stage:1 train_loss:8.723e-03 val_loss:8.733e-03  time:3m7s

 bg_pi:0.734 bg_ri:0.833 p_pi:0.737 p_ri:0.863 r_pi:0.776 r_ri:0.915 t_pi:0.900 t_ri:0.780
  5%|▍         | 12/250 [37:40<12:21:51, 187.02s/it]#epoch:12 stage:1 train_loss:8.690e-03 val_loss:8.699e-03  time:3m4s

 bg_pi:0.705 bg_ri:0.828 p_pi:0.755 p_ri:0.857 r_pi:0.780 r_ri:0.915 t_pi:0.898 t_ri:0.778
  5%|▌         | 13/250 [40:50<12:22:03, 187.86s/it]#epoch:13 stage:1 train_loss:8.660e-03 val_loss:8.664e-03  time:3m10s

 bg_pi:0.712 bg_ri:0.821 p_pi:0.763 p_ri:0.848 r_pi:0.786 r_ri:0.903 t_pi:0.892 t_ri:0.791
  6%|▌         | 14/250 [44:41<13:10:36, 201.00s/it]#epoch:14 stage:1 train_loss:8.638e-03 val_loss:8.638e-03  time:3m51s

 bg_pi:0.728 bg_ri:0.802 p_pi:0.777 p_ri:0.859 r_pi:0.784 r_ri:0.907 t_pi:0.891 t_ri:0.799
  6%|▌         | 15/250 [48:03<13:07:46, 201.13s/it]#epoch:15 stage:1 train_loss:8.605e-03 val_loss:8.668e-03  time:3m21s

 bg_pi:0.735 bg_ri:0.806 p_pi:0.758 p_ri:0.847 r_pi:0.789 r_ri:0.873 t_pi:0.877 t_ri:0.802
  6%|▋         | 16/250 [51:14<12:52:58, 198.20s/it]#epoch:16 stage:1 train_loss:8.581e-03 val_loss:8.612e-03  time:3m11s

 bg_pi:0.721 bg_ri:0.832 p_pi:0.764 p_ri:0.849 r_pi:0.791 r_ri:0.909 t_pi:0.896 t_ri:0.794
  7%|▋         | 17/250 [54:23<12:38:23, 195.29s/it]#epoch:17 stage:1 train_loss:8.552e-03 val_loss:8.646e-03  time:3m8s

 bg_pi:0.723 bg_ri:0.821 p_pi:0.759 p_ri:0.852 r_pi:0.764 r_ri:0.905 t_pi:0.892 t_ri:0.780
  7%|▋         | 18/250 [57:33<12:29:12, 193.76s/it]#epoch:18 stage:1 train_loss:8.533e-03 val_loss:8.562e-03  time:3m10s

 bg_pi:0.740 bg_ri:0.817 p_pi:0.764 p_ri:0.833 r_pi:0.816 r_ri:0.887 t_pi:0.883 t_ri:0.817
  8%|▊         | 19/250 [1:00:43<12:21:58, 192.72s/it]#epoch:19 stage:1 train_loss:8.509e-03 val_loss:8.589e-03  time:3m10s

 bg_pi:0.725 bg_ri:0.845 p_pi:0.769 p_ri:0.842 r_pi:0.780 r_ri:0.927 t_pi:0.904 t_ri:0.788
  8%|▊         | 20/250 [1:03:49<12:10:37, 190.60s/it]#epoch:20 stage:1 train_loss:8.488e-03 val_loss:8.564e-03  time:3m6s

 bg_pi:0.758 bg_ri:0.802 p_pi:0.773 p_ri:0.861 r_pi:0.811 r_ri:0.875 t_pi:0.882 t_ri:0.823
  8%|▊         | 21/250 [1:07:00<12:08:30, 190.88s/it]#epoch:21 stage:1 train_loss:8.468e-03 val_loss:8.560e-03  time:3m11s

 bg_pi:0.697 bg_ri:0.831 p_pi:0.750 p_ri:0.853 r_pi:0.777 r_ri:0.908 t_pi:0.894 t_ri:0.772
  9%|▉         | 22/250 [1:10:14<12:08:03, 191.60s/it]#epoch:22 stage:1 train_loss:8.444e-03 val_loss:8.541e-03  time:3m13s

 bg_pi:0.710 bg_ri:0.820 p_pi:0.771 p_ri:0.854 r_pi:0.777 r_ri:0.905 t_pi:0.893 t_ri:0.787
  9%|▉         | 23/250 [1:13:24<12:03:46, 191.31s/it]#epoch:23 stage:1 train_loss:8.440e-03 val_loss:8.546e-03  time:3m11s

 bg_pi:0.720 bg_ri:0.828 p_pi:0.750 p_ri:0.830 r_pi:0.778 r_ri:0.906 t_pi:0.890 t_ri:0.785
 10%|▉         | 24/250 [1:16:35<11:59:49, 191.10s/it]#epoch:24 stage:1 train_loss:8.417e-03 val_loss:8.494e-03  time:3m11s

 bg_pi:0.751 bg_ri:0.810 p_pi:0.789 p_ri:0.839 r_pi:0.791 r_ri:0.925 t_pi:0.897 t_ri:0.813
 10%|█         | 25/250 [1:19:39<11:48:53, 189.04s/it]#epoch:25 stage:1 train_loss:8.403e-03 val_loss:8.500e-03  time:3m4s

 bg_pi:0.743 bg_ri:0.803 p_pi:0.754 p_ri:0.847 r_pi:0.782 r_ri:0.909 t_pi:0.889 t_ri:0.796
 10%|█         | 26/250 [1:22:50<11:47:42, 189.56s/it]#epoch:26 stage:1 train_loss:8.377e-03 val_loss:8.477e-03  time:3m11s

 bg_pi:0.721 bg_ri:0.805 p_pi:0.757 p_ri:0.839 r_pi:0.806 r_ri:0.884 t_pi:0.879 t_ri:0.804
 11%|█         | 27/250 [1:25:57<11:41:30, 188.74s/it]#epoch:27 stage:1 train_loss:8.376e-03 val_loss:8.528e-03  time:3m7s

 bg_pi:0.703 bg_ri:0.835 p_pi:0.740 p_ri:0.829 r_pi:0.786 r_ri:0.913 t_pi:0.893 t_ri:0.778
 11%|█         | 28/250 [1:29:02<11:34:12, 187.62s/it]#epoch:28 stage:1 train_loss:8.362e-03 val_loss:8.480e-03  time:3m5s

 bg_pi:0.722 bg_ri:0.815 p_pi:0.750 p_ri:0.830 r_pi:0.793 r_ri:0.912 t_pi:0.889 t_ri:0.794
 12%|█▏        | 29/250 [1:32:07<11:27:59, 186.78s/it]#epoch:29 stage:1 train_loss:8.344e-03 val_loss:8.461e-03  time:3m5s

 bg_pi:0.738 bg_ri:0.822 p_pi:0.766 p_ri:0.838 r_pi:0.789 r_ri:0.887 t_pi:0.883 t_ri:0.802
 12%|█▏        | 30/250 [1:37:06<13:29:04, 220.66s/it]#epoch:30 stage:1 train_loss:8.332e-03 val_loss:8.460e-03  time:4m60s

 bg_pi:0.714 bg_ri:0.817 p_pi:0.767 p_ri:0.819 r_pi:0.815 r_ri:0.866 t_pi:0.872 t_ri:0.812
 12%|█▏        | 31/250 [1:40:31<13:07:51, 215.85s/it]#epoch:31 stage:1 train_loss:8.319e-03 val_loss:8.452e-03  time:3m25s

 bg_pi:0.756 bg_ri:0.804 p_pi:0.784 p_ri:0.827 r_pi:0.798 r_ri:0.905 t_pi:0.885 t_ri:0.817
 13%|█▎        | 32/250 [1:43:40<12:35:24, 207.91s/it]#epoch:32 stage:1 train_loss:8.313e-03 val_loss:8.457e-03  time:3m9s

 bg_pi:0.733 bg_ri:0.812 p_pi:0.763 p_ri:0.818 r_pi:0.803 r_ri:0.884 t_pi:0.877 t_ri:0.809
 13%|█▎        | 33/250 [1:46:52<12:13:53, 202.92s/it]#epoch:33 stage:1 train_loss:8.296e-03 val_loss:8.459e-03  time:3m11s

 bg_pi:0.736 bg_ri:0.810 p_pi:0.749 p_ri:0.829 r_pi:0.801 r_ri:0.871 t_pi:0.874 t_ri:0.805
 14%|█▎        | 34/250 [1:50:01<11:55:30, 198.75s/it]#epoch:34 stage:1 train_loss:8.286e-03 val_loss:8.444e-03  time:3m9s

 bg_pi:0.730 bg_ri:0.807 p_pi:0.748 p_ri:0.835 r_pi:0.799 r_ri:0.906 t_pi:0.887 t_ri:0.799
 14%|█▍        | 35/250 [1:53:11<11:42:45, 196.12s/it]#epoch:35 stage:1 train_loss:8.285e-03 val_loss:8.443e-03  time:3m10s

 bg_pi:0.737 bg_ri:0.826 p_pi:0.767 p_ri:0.829 r_pi:0.826 r_ri:0.883 t_pi:0.883 t_ri:0.820
 14%|█▍        | 36/250 [1:56:15<11:26:30, 192.48s/it]#epoch:36 stage:1 train_loss:8.264e-03 val_loss:8.414e-03  time:3m4s

 bg_pi:0.752 bg_ri:0.795 p_pi:0.763 p_ri:0.824 r_pi:0.807 r_ri:0.884 t_pi:0.875 t_ri:0.818
 15%|█▍        | 37/250 [1:59:25<11:21:21, 191.93s/it]#epoch:37 stage:1 train_loss:8.248e-03 val_loss:8.419e-03  time:3m11s

 bg_pi:0.772 bg_ri:0.772 p_pi:0.787 p_ri:0.846 r_pi:0.815 r_ri:0.865 t_pi:0.871 t_ri:0.838
 15%|█▌        | 38/250 [2:02:35<11:15:57, 191.31s/it]#epoch:38 stage:1 train_loss:8.234e-03 val_loss:8.419e-03  time:3m10s

 bg_pi:0.730 bg_ri:0.779 p_pi:0.770 p_ri:0.819 r_pi:0.816 r_ri:0.868 t_pi:0.866 t_ri:0.821
 16%|█▌        | 39/250 [2:05:46<11:11:55, 191.07s/it]#epoch:39 stage:1 train_loss:8.235e-03 val_loss:8.398e-03  time:3m10s

 bg_pi:0.746 bg_ri:0.787 p_pi:0.784 p_ri:0.841 r_pi:0.819 r_ri:0.858 t_pi:0.870 t_ri:0.830
 16%|█▌        | 40/250 [2:08:55<11:06:25, 190.41s/it]#epoch:40 stage:1 train_loss:8.226e-03 val_loss:8.423e-03  time:3m9s

 bg_pi:0.741 bg_ri:0.806 p_pi:0.765 p_ri:0.829 r_pi:0.803 r_ri:0.883 t_pi:0.879 t_ri:0.814
 16%|█▋        | 41/250 [2:12:06<11:04:33, 190.78s/it]#epoch:41 stage:1 train_loss:8.220e-03 val_loss:8.393e-03  time:3m12s

 bg_pi:0.766 bg_ri:0.769 p_pi:0.772 p_ri:0.843 r_pi:0.815 r_ri:0.849 t_pi:0.864 t_ri:0.832
 17%|█▋        | 42/250 [2:15:17<11:01:46, 190.90s/it]#epoch:42 stage:1 train_loss:8.219e-03 val_loss:8.412e-03  time:3m11s

 bg_pi:0.732 bg_ri:0.805 p_pi:0.777 p_ri:0.836 r_pi:0.811 r_ri:0.884 t_pi:0.880 t_ri:0.817
 17%|█▋        | 43/250 [2:18:26<10:56:31, 190.30s/it]#epoch:43 stage:1 train_loss:8.202e-03 val_loss:8.386e-03  time:3m9s

 bg_pi:0.757 bg_ri:0.789 p_pi:0.789 p_ri:0.812 r_pi:0.821 r_ri:0.864 t_pi:0.867 t_ri:0.837
 18%|█▊        | 44/250 [2:21:32<10:48:48, 188.97s/it]#epoch:44 stage:1 train_loss:8.201e-03 val_loss:8.419e-03  time:3m6s

 bg_pi:0.740 bg_ri:0.822 p_pi:0.765 p_ri:0.841 r_pi:0.783 r_ri:0.903 t_pi:0.891 t_ri:0.800
 18%|█▊        | 45/250 [2:24:41<10:45:53, 189.04s/it]#epoch:45 stage:1 train_loss:8.189e-03 val_loss:8.397e-03  time:3m9s

 bg_pi:0.750 bg_ri:0.763 p_pi:0.778 p_ri:0.829 r_pi:0.805 r_ri:0.871 t_pi:0.867 t_ri:0.825
 18%|█▊        | 46/250 [2:29:13<12:06:41, 213.73s/it]#epoch:46 stage:1 train_loss:8.183e-03 val_loss:8.402e-03  time:4m31s

 bg_pi:0.751 bg_ri:0.803 p_pi:0.777 p_ri:0.837 r_pi:0.800 r_ri:0.891 t_pi:0.883 t_ri:0.818
 19%|█▉        | 47/250 [2:32:24<11:40:34, 207.07s/it]#epoch:47 stage:1 train_loss:8.176e-03 val_loss:8.403e-03  time:3m11s

 bg_pi:0.757 bg_ri:0.771 p_pi:0.773 p_ri:0.843 r_pi:0.819 r_ri:0.850 t_pi:0.864 t_ri:0.831
 19%|█▉        | 48/250 [2:35:33<11:18:19, 201.48s/it]#epoch:48 stage:1 train_loss:8.165e-03 val_loss:8.385e-03  time:3m8s

 bg_pi:0.758 bg_ri:0.794 p_pi:0.770 p_ri:0.831 r_pi:0.806 r_ri:0.877 t_pi:0.874 t_ri:0.822
 20%|█▉        | 49/250 [2:38:37<10:57:58, 196.41s/it]#epoch:49 stage:1 train_loss:8.164e-03 val_loss:8.403e-03  time:3m5s

 bg_pi:0.729 bg_ri:0.823 p_pi:0.759 p_ri:0.832 r_pi:0.799 r_ri:0.904 t_pi:0.890 t_ri:0.803
 20%|██        | 50/250 [2:41:48<10:49:34, 194.87s/it]#epoch:50 stage:1 train_loss:8.156e-03 val_loss:8.377e-03  time:3m11s

 bg_pi:0.755 bg_ri:0.800 p_pi:0.759 p_ri:0.826 r_pi:0.814 r_ri:0.881 t_pi:0.876 t_ri:0.821
 20%|██        | 51/250 [2:44:52<10:35:09, 191.51s/it]#epoch:51 stage:1 train_loss:8.141e-03 val_loss:8.367e-03  time:3m4s

 bg_pi:0.757 bg_ri:0.782 p_pi:0.783 p_ri:0.830 r_pi:0.818 r_ri:0.885 t_pi:0.875 t_ri:0.829
 21%|██        | 52/250 [2:48:02<10:29:59, 190.91s/it]#epoch:52 stage:1 train_loss:8.142e-03 val_loss:8.406e-03  time:3m9s

 bg_pi:0.766 bg_ri:0.746 p_pi:0.781 p_ri:0.804 r_pi:0.816 r_ri:0.832 t_pi:0.848 t_ri:0.842
 21%|██        | 53/250 [2:51:14<10:28:37, 191.46s/it]#epoch:53 stage:1 train_loss:8.144e-03 val_loss:8.423e-03  time:3m13s

 bg_pi:0.741 bg_ri:0.828 p_pi:0.744 p_ri:0.856 r_pi:0.797 r_ri:0.908 t_pi:0.897 t_ri:0.798
 22%|██▏       | 54/250 [2:54:20<10:19:22, 189.60s/it]#epoch:54 stage:1 train_loss:8.144e-03 val_loss:8.395e-03  time:3m5s

 bg_pi:0.729 bg_ri:0.814 p_pi:0.763 p_ri:0.831 r_pi:0.808 r_ri:0.884 t_pi:0.880 t_ri:0.810
 22%|██▏       | 55/250 [2:57:29<10:16:15, 189.62s/it]#epoch:55 stage:1 train_loss:8.139e-03 val_loss:8.366e-03  time:3m10s

 bg_pi:0.755 bg_ri:0.802 p_pi:0.787 p_ri:0.826 r_pi:0.813 r_ri:0.865 t_pi:0.872 t_ri:0.830
 22%|██▏       | 56/250 [3:00:39<10:13:14, 189.66s/it]#epoch:56 stage:1 train_loss:8.132e-03 val_loss:8.381e-03  time:3m10s

 bg_pi:0.756 bg_ri:0.779 p_pi:0.769 p_ri:0.829 r_pi:0.812 r_ri:0.866 t_pi:0.868 t_ri:0.826
 23%|██▎       | 57/250 [3:03:48<10:09:27, 189.47s/it]#epoch:57 stage:1 train_loss:8.118e-03 val_loss:8.370e-03  time:3m9s

 bg_pi:0.756 bg_ri:0.784 p_pi:0.776 p_ri:0.848 r_pi:0.824 r_ri:0.866 t_pi:0.874 t_ri:0.832
 23%|██▎       | 58/250 [3:06:58<10:06:28, 189.52s/it]#epoch:58 stage:1 train_loss:8.117e-03 val_loss:8.383e-03  time:3m10s

 bg_pi:0.749 bg_ri:0.793 p_pi:0.764 p_ri:0.845 r_pi:0.800 r_ri:0.875 t_pi:0.875 t_ri:0.813
 24%|██▎       | 59/250 [3:10:03<9:59:00, 188.17s/it] #epoch:59 stage:1 train_loss:8.111e-03 val_loss:8.375e-03  time:3m5s

 bg_pi:0.774 bg_ri:0.772 p_pi:0.779 p_ri:0.833 r_pi:0.814 r_ri:0.872 t_pi:0.871 t_ri:0.835
 24%|██▍       | 60/250 [3:13:09<9:54:04, 187.60s/it]#epoch:60 stage:1 train_loss:8.100e-03 val_loss:8.363e-03  time:3m6s

 bg_pi:0.759 bg_ri:0.787 p_pi:0.794 p_ri:0.837 r_pi:0.822 r_ri:0.863 t_pi:0.872 t_ri:0.838
 24%|██▍       | 61/250 [3:16:19<9:53:19, 188.36s/it]#epoch:61 stage:1 train_loss:8.114e-03 val_loss:8.376e-03  time:3m10s

 bg_pi:0.733 bg_ri:0.821 p_pi:0.782 p_ri:0.837 r_pi:0.799 r_ri:0.890 t_pi:0.885 t_ri:0.811
 25%|██▍       | 62/250 [3:21:25<11:41:02, 223.74s/it]#epoch:62 stage:1 train_loss:8.109e-03 val_loss:8.364e-03  time:5m6s

 bg_pi:0.745 bg_ri:0.781 p_pi:0.773 p_ri:0.820 r_pi:0.805 r_ri:0.872 t_pi:0.868 t_ri:0.820
 25%|██▌       | 63/250 [3:24:44<11:13:36, 216.13s/it]#epoch:63 stage:1 train_loss:8.104e-03 val_loss:8.371e-03  time:3m18s

 bg_pi:0.750 bg_ri:0.802 p_pi:0.780 p_ri:0.835 r_pi:0.801 r_ri:0.872 t_pi:0.875 t_ri:0.819
 26%|██▌       | 64/250 [3:27:49<10:41:18, 206.87s/it]#epoch:64 stage:1 train_loss:8.095e-03 val_loss:8.355e-03  time:3m5s

 bg_pi:0.760 bg_ri:0.773 p_pi:0.775 p_ri:0.812 r_pi:0.830 r_ri:0.852 t_pi:0.860 t_ri:0.839
 26%|██▌       | 65/250 [3:30:56<10:19:22, 200.88s/it]#epoch:65 stage:1 train_loss:8.097e-03 val_loss:8.352e-03  time:3m7s

 bg_pi:0.760 bg_ri:0.770 p_pi:0.796 p_ri:0.836 r_pi:0.821 r_ri:0.847 t_pi:0.862 t_ri:0.840
 26%|██▋       | 66/250 [3:34:05<10:05:20, 197.39s/it]#epoch:66 stage:1 train_loss:8.097e-03 val_loss:8.378e-03  time:3m9s

 bg_pi:0.780 bg_ri:0.784 p_pi:0.781 p_ri:0.834 r_pi:0.810 r_ri:0.863 t_pi:0.870 t_ri:0.835
 27%|██▋       | 67/250 [3:37:15<9:55:27, 195.23s/it] #epoch:67 stage:1 train_loss:8.075e-03 val_loss:8.371e-03  time:3m10s

 bg_pi:0.751 bg_ri:0.794 p_pi:0.771 p_ri:0.850 r_pi:0.808 r_ri:0.881 t_pi:0.880 t_ri:0.820
 27%|██▋       | 68/250 [3:40:26<9:47:44, 193.76s/it]#epoch:68 stage:1 train_loss:8.076e-03 val_loss:8.370e-03  time:3m10s

 bg_pi:0.748 bg_ri:0.794 p_pi:0.776 p_ri:0.834 r_pi:0.814 r_ri:0.849 t_pi:0.865 t_ri:0.826
 28%|██▊       | 69/250 [3:43:32<9:37:58, 191.59s/it]#epoch:69 stage:1 train_loss:8.145e-03 val_loss:8.400e-03  time:3m6s

 bg_pi:0.754 bg_ri:0.779 p_pi:0.756 p_ri:0.819 r_pi:0.804 r_ri:0.885 t_pi:0.873 t_ri:0.819
 28%|██▊       | 70/250 [3:46:43<9:34:09, 191.39s/it]#epoch:70 stage:1 train_loss:8.142e-03 val_loss:8.422e-03  time:3m11s

 bg_pi:0.737 bg_ri:0.785 p_pi:0.778 p_ri:0.817 r_pi:0.803 r_ri:0.869 t_pi:0.868 t_ri:0.819
 28%|██▊       | 71/250 [3:49:49<9:26:13, 189.79s/it]#epoch:71 stage:1 train_loss:8.174e-03 val_loss:8.370e-03  time:3m6s

 bg_pi:0.764 bg_ri:0.759 p_pi:0.772 p_ri:0.843 r_pi:0.836 r_ri:0.847 t_pi:0.862 t_ri:0.842
 29%|██▉       | 72/250 [3:53:00<9:23:50, 190.06s/it]#epoch:72 stage:1 train_loss:8.134e-03 val_loss:8.399e-03  time:3m11s

 bg_pi:0.761 bg_ri:0.772 p_pi:0.766 p_ri:0.835 r_pi:0.818 r_ri:0.849 t_pi:0.863 t_ri:0.831
 29%|██▉       | 73/250 [3:56:05<9:16:16, 188.57s/it]#epoch:73 stage:1 train_loss:8.158e-03 val_loss:8.351e-03  time:3m5s

 bg_pi:0.764 bg_ri:0.759 p_pi:0.799 p_ri:0.814 r_pi:0.829 r_ri:0.864 t_pi:0.863 t_ri:0.847
 30%|██▉       | 74/250 [3:59:14<9:13:14, 188.61s/it]#epoch:74 stage:1 train_loss:8.109e-03 val_loss:8.367e-03  time:3m9s

 bg_pi:0.760 bg_ri:0.803 p_pi:0.777 p_ri:0.843 r_pi:0.825 r_ri:0.865 t_pi:0.876 t_ri:0.832
 30%|███       | 75/250 [4:02:18<9:05:52, 187.16s/it]#epoch:75 stage:1 train_loss:8.093e-03 val_loss:8.375e-03  time:3m4s

 bg_pi:0.764 bg_ri:0.786 p_pi:0.758 p_ri:0.847 r_pi:0.814 r_ri:0.868 t_pi:0.873 t_ri:0.824
 30%|███       | 76/250 [4:05:26<9:04:09, 187.64s/it]#epoch:76 stage:1 train_loss:8.058e-03 val_loss:8.349e-03  time:3m9s

 bg_pi:0.757 bg_ri:0.796 p_pi:0.766 p_ri:0.835 r_pi:0.817 r_ri:0.866 t_pi:0.873 t_ri:0.826
 31%|███       | 77/250 [4:08:35<9:02:04, 188.00s/it]#epoch:77 stage:1 train_loss:8.053e-03 val_loss:8.485e-03  time:3m9s

 bg_pi:0.790 bg_ri:0.677 p_pi:0.789 p_ri:0.829 r_pi:0.821 r_ri:0.806 t_pi:0.834 t_ri:0.856
 31%|███       | 78/250 [4:14:03<10:59:01, 229.89s/it]#epoch:78 stage:1 train_loss:8.109e-03 val_loss:8.379e-03  time:5m28s

 bg_pi:0.741 bg_ri:0.807 p_pi:0.751 p_ri:0.846 r_pi:0.796 r_ri:0.878 t_pi:0.879 t_ri:0.803
 32%|███▏      | 79/250 [4:17:20<10:27:03, 220.02s/it]#epoch:79 stage:1 train_loss:8.064e-03 val_loss:8.375e-03  time:3m17s

 bg_pi:0.742 bg_ri:0.795 p_pi:0.767 p_ri:0.821 r_pi:0.804 r_ri:0.879 t_pi:0.873 t_ri:0.816
 32%|███▏      | 80/250 [4:20:29<9:57:27, 210.87s/it] #epoch:80 stage:1 train_loss:8.035e-03 val_loss:8.355e-03  time:3m9s

 bg_pi:0.766 bg_ri:0.772 p_pi:0.776 p_ri:0.819 r_pi:0.828 r_ri:0.834 t_pi:0.854 t_ri:0.840
 32%|███▏      | 81/250 [4:23:38<9:35:06, 204.18s/it]#epoch:81 stage:1 train_loss:8.028e-03 val_loss:8.335e-03  time:3m9s

 bg_pi:0.757 bg_ri:0.776 p_pi:0.774 p_ri:0.816 r_pi:0.820 r_ri:0.861 t_pi:0.864 t_ri:0.832
 33%|███▎      | 82/250 [4:26:48<9:19:55, 199.97s/it]#epoch:82 stage:1 train_loss:8.013e-03 val_loss:8.358e-03  time:3m10s

 bg_pi:0.763 bg_ri:0.767 p_pi:0.786 p_ri:0.826 r_pi:0.822 r_ri:0.856 t_pi:0.864 t_ri:0.840
 33%|███▎      | 83/250 [4:29:59<9:08:46, 197.17s/it]#epoch:83 stage:1 train_loss:8.009e-03 val_loss:8.355e-03  time:3m11s

 bg_pi:0.749 bg_ri:0.780 p_pi:0.776 p_ri:0.831 r_pi:0.807 r_ri:0.862 t_pi:0.867 t_ri:0.823
 34%|███▎      | 84/250 [4:33:07<8:58:00, 194.46s/it]#epoch:84 stage:1 train_loss:8.003e-03 val_loss:8.340e-03  time:3m8s

 bg_pi:0.770 bg_ri:0.761 p_pi:0.796 p_ri:0.838 r_pi:0.827 r_ri:0.845 t_pi:0.862 t_ri:0.847
 34%|███▍      | 85/250 [4:36:17<8:51:34, 193.30s/it]#epoch:85 stage:1 train_loss:8.001e-03 val_loss:8.353e-03  time:3m11s

 bg_pi:0.762 bg_ri:0.790 p_pi:0.782 p_ri:0.842 r_pi:0.815 r_ri:0.852 t_pi:0.869 t_ri:0.833
 34%|███▍      | 86/250 [4:39:24<8:42:32, 191.17s/it]#epoch:86 stage:1 train_loss:8.011e-03 val_loss:8.374e-03  time:3m6s

 bg_pi:0.733 bg_ri:0.791 p_pi:0.771 p_ri:0.802 r_pi:0.796 r_ri:0.892 t_pi:0.872 t_ri:0.810
 35%|███▍      | 87/250 [4:42:32<8:36:59, 190.31s/it]#epoch:87 stage:1 train_loss:8.046e-03 val_loss:8.374e-03  time:3m8s

 bg_pi:0.729 bg_ri:0.807 p_pi:0.755 p_ri:0.833 r_pi:0.798 r_ri:0.901 t_pi:0.885 t_ri:0.802
 35%|███▌      | 88/250 [4:45:42<8:33:34, 190.21s/it]#epoch:88 stage:1 train_loss:8.031e-03 val_loss:8.374e-03  time:3m10s

 bg_pi:0.777 bg_ri:0.731 p_pi:0.797 p_ri:0.809 r_pi:0.835 r_ri:0.834 t_pi:0.848 t_ri:0.857
 36%|███▌      | 89/250 [4:48:46<8:25:08, 188.25s/it]#epoch:89 stage:1 train_loss:8.107e-03 val_loss:8.376e-03  time:3m4s

 bg_pi:0.737 bg_ri:0.806 p_pi:0.748 p_ri:0.840 r_pi:0.812 r_ri:0.883 t_pi:0.880 t_ri:0.809
 36%|███▌      | 90/250 [4:51:51<8:19:59, 187.50s/it]#epoch:90 stage:1 train_loss:8.135e-03 val_loss:8.654e-03  time:3m6s

 bg_pi:0.811 bg_ri:0.587 p_pi:0.798 p_ri:0.825 r_pi:0.853 r_ri:0.764 t_pi:0.810 t_ri:0.885
 36%|███▋      | 91/250 [4:55:00<8:18:12, 188.00s/it]#epoch:91 stage:1 train_loss:8.235e-03 val_loss:8.354e-03  time:3m9s

 bg_pi:0.777 bg_ri:0.731 p_pi:0.800 p_ri:0.814 r_pi:0.827 r_ri:0.840 t_pi:0.851 t_ri:0.854
 37%|███▋      | 92/250 [4:58:11<8:17:26, 188.90s/it]#epoch:92 stage:1 train_loss:8.156e-03 val_loss:8.327e-03  time:3m11s

 bg_pi:0.751 bg_ri:0.807 p_pi:0.768 p_ri:0.842 r_pi:0.813 r_ri:0.868 t_pi:0.878 t_ri:0.824
 37%|███▋      | 93/250 [5:01:23<8:16:32, 189.76s/it]#epoch:93 stage:1 train_loss:8.092e-03 val_loss:8.369e-03  time:3m12s

 bg_pi:0.739 bg_ri:0.808 p_pi:0.763 p_ri:0.839 r_pi:0.797 r_ri:0.894 t_pi:0.884 t_ri:0.806
 38%|███▊      | 94/250 [5:06:08<9:27:36, 218.31s/it]#epoch:94 stage:1 train_loss:8.044e-03 val_loss:8.323e-03  time:4m45s

 bg_pi:0.747 bg_ri:0.792 p_pi:0.772 p_ri:0.806 r_pi:0.817 r_ri:0.859 t_pi:0.864 t_ri:0.828
 38%|███▊      | 95/250 [5:09:13<8:57:42, 208.14s/it]#epoch:95 stage:1 train_loss:8.013e-03 val_loss:8.322e-03  time:3m4s

 bg_pi:0.768 bg_ri:0.776 p_pi:0.785 p_ri:0.818 r_pi:0.814 r_ri:0.857 t_pi:0.863 t_ri:0.836
 38%|███▊      | 96/250 [5:12:17<8:36:10, 201.11s/it]#epoch:96 stage:1 train_loss:8.005e-03 val_loss:8.334e-03  time:3m5s

 bg_pi:0.777 bg_ri:0.763 p_pi:0.787 p_ri:0.819 r_pi:0.824 r_ri:0.847 t_pi:0.859 t_ri:0.846
 39%|███▉      | 97/250 [5:15:23<8:21:14, 196.57s/it]#epoch:97 stage:1 train_loss:7.995e-03 val_loss:8.348e-03  time:3m6s

 bg_pi:0.743 bg_ri:0.806 p_pi:0.768 p_ri:0.827 r_pi:0.809 r_ri:0.885 t_pi:0.879 t_ri:0.817
 39%|███▉      | 98/250 [5:18:29<8:10:03, 193.45s/it]#epoch:98 stage:1 train_loss:7.984e-03 val_loss:8.328e-03  time:3m6s

 bg_pi:0.772 bg_ri:0.760 p_pi:0.787 p_ri:0.825 r_pi:0.823 r_ri:0.865 t_pi:0.865 t_ri:0.842
 40%|███▉      | 99/250 [5:21:41<8:05:24, 192.88s/it]#epoch:99 stage:1 train_loss:7.977e-03 val_loss:8.354e-03  time:3m12s

 bg_pi:0.764 bg_ri:0.776 p_pi:0.768 p_ri:0.821 r_pi:0.823 r_ri:0.827 t_pi:0.854 t_ri:0.837
 40%|████      | 100/250 [5:24:52<8:01:03, 192.42s/it]#epoch:100 stage:1 train_loss:7.983e-03 val_loss:8.349e-03  time:3m11s

 bg_pi:0.744 bg_ri:0.787 p_pi:0.774 p_ri:0.835 r_pi:0.807 r_ri:0.873 t_pi:0.872 t_ri:0.820
 40%|████      | 101/250 [5:27:57<7:52:03, 190.09s/it]#epoch:101 stage:1 train_loss:7.977e-03 val_loss:8.350e-03  time:3m5s

 bg_pi:0.757 bg_ri:0.804 p_pi:0.764 p_ri:0.843 r_pi:0.810 r_ri:0.879 t_pi:0.880 t_ri:0.821
 41%|████      | 102/250 [5:31:09<7:50:01, 190.55s/it]#epoch:102 stage:1 train_loss:7.977e-03 val_loss:8.369e-03  time:3m12s

 bg_pi:0.743 bg_ri:0.805 p_pi:0.774 p_ri:0.820 r_pi:0.812 r_ri:0.862 t_pi:0.871 t_ri:0.824
 41%|████      | 103/250 [5:34:13<7:42:28, 188.77s/it]#epoch:103 stage:1 train_loss:7.974e-03 val_loss:8.353e-03  time:3m5s

 bg_pi:0.758 bg_ri:0.797 p_pi:0.781 p_ri:0.839 r_pi:0.822 r_ri:0.864 t_pi:0.874 t_ri:0.834
 42%|████▏     | 104/250 [5:37:23<7:40:02, 189.06s/it]#epoch:104 stage:1 train_loss:7.969e-03 val_loss:8.333e-03  time:3m10s

 bg_pi:0.762 bg_ri:0.775 p_pi:0.776 p_ri:0.821 r_pi:0.815 r_ri:0.866 t_pi:0.866 t_ri:0.832
 42%|████▏     | 105/250 [5:40:33<7:37:59, 189.51s/it]#epoch:105 stage:1 train_loss:7.977e-03 val_loss:8.336e-03  time:3m11s

 bg_pi:0.753 bg_ri:0.786 p_pi:0.768 p_ri:0.814 r_pi:0.807 r_ri:0.868 t_pi:0.866 t_ri:0.822
 42%|████▏     | 106/250 [5:43:39<7:32:02, 188.35s/it]#epoch:106 stage:1 train_loss:7.964e-03 val_loss:8.331e-03  time:3m6s

 bg_pi:0.750 bg_ri:0.797 p_pi:0.773 p_ri:0.835 r_pi:0.812 r_ri:0.865 t_pi:0.873 t_ri:0.825
 43%|████▎     | 107/250 [5:46:44<7:26:18, 187.26s/it]#epoch:107 stage:1 train_loss:7.954e-03 val_loss:8.346e-03  time:3m5s

 bg_pi:0.760 bg_ri:0.763 p_pi:0.777 p_ri:0.825 r_pi:0.824 r_ri:0.840 t_pi:0.857 t_ri:0.838
 43%|████▎     | 108/250 [5:49:55<7:25:39, 188.30s/it]#epoch:108 stage:1 train_loss:7.942e-03 val_loss:8.349e-03  time:3m11s

 bg_pi:0.758 bg_ri:0.773 p_pi:0.776 p_ri:0.827 r_pi:0.817 r_ri:0.843 t_pi:0.859 t_ri:0.833
 44%|████▎     | 109/250 [5:53:00<7:20:07, 187.29s/it]#epoch:109 stage:1 train_loss:7.935e-03 val_loss:8.359e-03  time:3m5s

 bg_pi:0.763 bg_ri:0.775 p_pi:0.774 p_ri:0.834 r_pi:0.819 r_ri:0.837 t_pi:0.860 t_ri:0.835
 44%|████▍     | 110/250 [5:57:53<8:31:20, 219.15s/it]#epoch:110 stage:1 train_loss:7.929e-03 val_loss:8.350e-03  time:4m53s

 bg_pi:0.761 bg_ri:0.780 p_pi:0.768 p_ri:0.828 r_pi:0.817 r_ri:0.854 t_pi:0.865 t_ri:0.831
 44%|████▍     | 111/250 [6:01:07<8:10:30, 211.73s/it]#epoch:111 stage:1 train_loss:7.926e-03 val_loss:8.367e-03  time:3m14s

 bg_pi:0.753 bg_ri:0.793 p_pi:0.768 p_ri:0.838 r_pi:0.821 r_ri:0.858 t_pi:0.869 t_ri:0.827
 45%|████▍     | 112/250 [6:04:19<7:52:44, 205.54s/it]#epoch:112 stage:1 train_loss:7.926e-03 val_loss:8.343e-03  time:3m11s

 bg_pi:0.762 bg_ri:0.772 p_pi:0.783 p_ri:0.840 r_pi:0.817 r_ri:0.856 t_pi:0.867 t_ri:0.835
 45%|████▌     | 113/250 [6:07:24<7:35:26, 199.46s/it]#epoch:113 stage:1 train_loss:7.918e-03 val_loss:8.342e-03  time:3m5s

 bg_pi:0.755 bg_ri:0.788 p_pi:0.774 p_ri:0.844 r_pi:0.812 r_ri:0.863 t_pi:0.872 t_ri:0.826
 46%|████▌     | 114/250 [6:10:32<7:24:43, 196.20s/it]#epoch:114 stage:1 train_loss:7.915e-03 val_loss:8.356e-03  time:3m9s

 bg_pi:0.758 bg_ri:0.782 p_pi:0.770 p_ri:0.845 r_pi:0.810 r_ri:0.851 t_pi:0.867 t_ri:0.827
 46%|████▌     | 115/250 [6:13:44<7:18:16, 194.79s/it]#epoch:115 stage:1 train_loss:7.920e-03 val_loss:8.366e-03  time:3m11s

 bg_pi:0.764 bg_ri:0.776 p_pi:0.770 p_ri:0.852 r_pi:0.830 r_ri:0.836 t_pi:0.863 t_ri:0.837
 46%|████▋     | 116/250 [6:16:54<7:12:11, 193.52s/it]#epoch:116 stage:1 train_loss:7.915e-03 val_loss:8.339e-03  time:3m10s

 bg_pi:0.776 bg_ri:0.766 p_pi:0.781 p_ri:0.827 r_pi:0.834 r_ri:0.836 t_pi:0.858 t_ri:0.849
 47%|████▋     | 117/250 [6:20:04<7:06:14, 192.29s/it]#epoch:117 stage:1 train_loss:7.907e-03 val_loss:8.366e-03  time:3m9s

 bg_pi:0.760 bg_ri:0.767 p_pi:0.772 p_ri:0.851 r_pi:0.822 r_ri:0.842 t_pi:0.862 t_ri:0.834
 47%|████▋     | 118/250 [6:25:26<8:28:50, 231.29s/it]#epoch:118 stage:1 train_loss:7.904e-03 val_loss:8.358e-03  time:5m22s

 bg_pi:0.753 bg_ri:0.784 p_pi:0.772 p_ri:0.830 r_pi:0.818 r_ri:0.841 t_pi:0.861 t_ri:0.830
 48%|████▊     | 119/250 [6:28:38<7:59:15, 219.51s/it]#epoch:119 stage:1 train_loss:7.904e-03 val_loss:8.349e-03  time:3m12s

 bg_pi:0.767 bg_ri:0.751 p_pi:0.783 p_ri:0.828 r_pi:0.823 r_ri:0.843 t_pi:0.857 t_ri:0.842
 48%|████▊     | 120/250 [6:31:50<7:37:27, 211.13s/it]#epoch:120 stage:1 train_loss:7.904e-03 val_loss:8.349e-03  time:3m12s

 bg_pi:0.763 bg_ri:0.777 p_pi:0.777 p_ri:0.836 r_pi:0.817 r_ri:0.843 t_pi:0.862 t_ri:0.833
 48%|████▊     | 121/250 [6:34:59<7:19:56, 204.62s/it]#epoch:121 stage:1 train_loss:7.901e-03 val_loss:8.333e-03  time:3m9s

 bg_pi:0.769 bg_ri:0.780 p_pi:0.777 p_ri:0.836 r_pi:0.821 r_ri:0.848 t_pi:0.865 t_ri:0.838
 49%|████▉     | 122/250 [6:38:09<7:06:51, 200.09s/it]#epoch:122 stage:1 train_loss:7.906e-03 val_loss:8.354e-03  time:3m9s

 bg_pi:0.757 bg_ri:0.793 p_pi:0.771 p_ri:0.847 r_pi:0.811 r_ri:0.873 t_pi:0.876 t_ri:0.824
 49%|████▉     | 123/250 [6:41:21<6:58:17, 197.62s/it]#epoch:123 stage:1 train_loss:7.899e-03 val_loss:8.356e-03  time:3m12s

 bg_pi:0.759 bg_ri:0.794 p_pi:0.774 p_ri:0.830 r_pi:0.817 r_ri:0.853 t_pi:0.867 t_ri:0.830
 50%|████▉     | 124/250 [6:44:31<6:50:29, 195.48s/it]#epoch:124 stage:1 train_loss:7.896e-03 val_loss:8.352e-03  time:3m10s

 bg_pi:0.758 bg_ri:0.766 p_pi:0.781 p_ri:0.838 r_pi:0.822 r_ri:0.858 t_pi:0.865 t_ri:0.835
 50%|█████     | 125/250 [6:49:02<7:34:11, 218.01s/it]#epoch:125 stage:1 train_loss:7.899e-03 val_loss:8.346e-03  time:4m31s

 bg_pi:0.771 bg_ri:0.760 p_pi:0.783 p_ri:0.835 r_pi:0.825 r_ri:0.859 t_pi:0.865 t_ri:0.841
 50%|█████     | 126/250 [6:52:13<7:14:17, 210.14s/it]#epoch:126 stage:1 train_loss:7.902e-03 val_loss:8.349e-03  time:3m12s

 bg_pi:0.760 bg_ri:0.782 p_pi:0.770 p_ri:0.829 r_pi:0.823 r_ri:0.849 t_pi:0.864 t_ri:0.834
 51%|█████     | 127/250 [6:55:27<7:00:32, 205.15s/it]#epoch:127 stage:1 train_loss:7.899e-03 val_loss:8.362e-03  time:3m13s

 bg_pi:0.754 bg_ri:0.762 p_pi:0.785 p_ri:0.842 r_pi:0.815 r_ri:0.853 t_pi:0.865 t_ri:0.834
 51%|█████     | 128/250 [6:58:39<6:49:17, 201.29s/it]#epoch:128 stage:1 train_loss:7.895e-03 val_loss:8.339e-03  time:3m12s

 bg_pi:0.767 bg_ri:0.777 p_pi:0.773 p_ri:0.826 r_pi:0.818 r_ri:0.850 t_pi:0.862 t_ri:0.834
 52%|█████▏    | 129/250 [7:01:51<6:40:01, 198.36s/it]#epoch:129 stage:1 train_loss:7.887e-03 val_loss:8.365e-03  time:3m11s

 bg_pi:0.765 bg_ri:0.776 p_pi:0.769 p_ri:0.845 r_pi:0.816 r_ri:0.847 t_pi:0.865 t_ri:0.831
 52%|█████▏    | 130/250 [7:05:01<6:31:46, 195.89s/it]#epoch:130 stage:1 train_loss:7.896e-03 val_loss:8.370e-03  time:3m10s

 bg_pi:0.755 bg_ri:0.776 p_pi:0.782 p_ri:0.831 r_pi:0.817 r_ri:0.859 t_pi:0.867 t_ri:0.833
 52%|█████▏    | 131/250 [7:08:10<6:24:48, 194.02s/it]#epoch:131 stage:1 train_loss:7.905e-03 val_loss:8.345e-03  time:3m10s

 bg_pi:0.758 bg_ri:0.779 p_pi:0.781 p_ri:0.843 r_pi:0.814 r_ri:0.867 t_pi:0.872 t_ri:0.830
 53%|█████▎    | 132/250 [7:11:21<6:19:36, 193.02s/it]#epoch:132 stage:1 train_loss:7.908e-03 val_loss:8.365e-03  time:3m11s

 bg_pi:0.764 bg_ri:0.783 p_pi:0.783 p_ri:0.842 r_pi:0.816 r_ri:0.842 t_pi:0.865 t_ri:0.836
 53%|█████▎    | 133/250 [7:14:33<6:15:36, 192.62s/it]#epoch:133 stage:1 train_loss:7.892e-03 val_loss:8.347e-03  time:3m12s

 bg_pi:0.769 bg_ri:0.767 p_pi:0.777 p_ri:0.836 r_pi:0.825 r_ri:0.845 t_pi:0.861 t_ri:0.840
 54%|█████▎    | 134/250 [7:17:38<6:07:48, 190.24s/it]#epoch:134 stage:1 train_loss:7.883e-03 val_loss:8.360e-03  time:3m5s

 bg_pi:0.755 bg_ri:0.789 p_pi:0.769 p_ri:0.835 r_pi:0.814 r_ri:0.857 t_pi:0.868 t_ri:0.826
 54%|█████▍    | 135/250 [7:20:51<6:06:16, 191.10s/it]#epoch:135 stage:1 train_loss:7.880e-03 val_loss:8.371e-03  time:3m13s

 bg_pi:0.757 bg_ri:0.775 p_pi:0.776 p_ri:0.843 r_pi:0.820 r_ri:0.842 t_pi:0.863 t_ri:0.834
 54%|█████▍    | 136/250 [7:23:58<6:01:00, 190.00s/it]#epoch:136 stage:1 train_loss:7.878e-03 val_loss:8.368e-03  time:3m7s

 bg_pi:0.769 bg_ri:0.749 p_pi:0.785 p_ri:0.841 r_pi:0.821 r_ri:0.842 t_pi:0.859 t_ri:0.842
 55%|█████▍    | 137/250 [7:27:08<5:57:53, 190.03s/it]#epoch:137 stage:1 train_loss:7.879e-03 val_loss:8.366e-03  time:3m10s

 bg_pi:0.757 bg_ri:0.764 p_pi:0.772 p_ri:0.830 r_pi:0.819 r_ri:0.850 t_pi:0.861 t_ri:0.834
 55%|█████▌    | 138/250 [7:30:19<5:55:13, 190.30s/it]#epoch:138 stage:1 train_loss:7.876e-03 val_loss:8.343e-03  time:3m11s

 bg_pi:0.765 bg_ri:0.756 p_pi:0.781 p_ri:0.837 r_pi:0.827 r_ri:0.843 t_pi:0.859 t_ri:0.841
 56%|█████▌    | 139/250 [7:33:29<5:51:54, 190.22s/it]#epoch:139 stage:1 train_loss:7.867e-03 val_loss:8.363e-03  time:3m10s

 bg_pi:0.752 bg_ri:0.789 p_pi:0.770 p_ri:0.829 r_pi:0.819 r_ri:0.855 t_pi:0.867 t_ri:0.829
 56%|█████▌    | 140/250 [7:36:40<5:49:07, 190.43s/it]#epoch:140 stage:1 train_loss:7.865e-03 val_loss:8.353e-03  time:3m11s

 bg_pi:0.763 bg_ri:0.765 p_pi:0.783 p_ri:0.837 r_pi:0.823 r_ri:0.843 t_pi:0.861 t_ri:0.839
 56%|█████▋    | 141/250 [7:40:25<6:04:51, 200.84s/it]#epoch:141 stage:1 train_loss:7.861e-03 val_loss:8.372e-03  time:3m45s

 bg_pi:0.769 bg_ri:0.772 p_pi:0.781 p_ri:0.843 r_pi:0.821 r_ri:0.842 t_pi:0.863 t_ri:0.839
 57%|█████▋    | 142/250 [7:43:37<5:56:25, 198.01s/it]#epoch:142 stage:1 train_loss:7.860e-03 val_loss:8.360e-03  time:3m11s

 bg_pi:0.770 bg_ri:0.761 p_pi:0.770 p_ri:0.840 r_pi:0.819 r_ri:0.857 t_pi:0.865 t_ri:0.835
 57%|█████▋    | 143/250 [7:46:48<5:49:26, 195.95s/it]#epoch:143 stage:1 train_loss:7.856e-03 val_loss:8.361e-03  time:3m11s

 bg_pi:0.759 bg_ri:0.776 p_pi:0.780 p_ri:0.829 r_pi:0.820 r_ri:0.844 t_pi:0.862 t_ri:0.836
 58%|█████▊    | 144/250 [7:50:00<5:43:57, 194.69s/it]#epoch:144 stage:1 train_loss:7.861e-03 val_loss:8.372e-03  time:3m12s

 bg_pi:0.759 bg_ri:0.784 p_pi:0.769 p_ri:0.839 r_pi:0.821 r_ri:0.851 t_pi:0.866 t_ri:0.831
 58%|█████▊    | 145/250 [7:53:08<5:37:36, 192.92s/it]#epoch:145 stage:1 train_loss:7.859e-03 val_loss:8.358e-03  time:3m9s

 bg_pi:0.760 bg_ri:0.786 p_pi:0.776 p_ri:0.841 r_pi:0.820 r_ri:0.849 t_pi:0.866 t_ri:0.832
 58%|█████▊    | 146/250 [7:56:18<5:32:32, 191.85s/it]#epoch:146 stage:1 train_loss:7.855e-03 val_loss:8.365e-03  time:3m9s

 bg_pi:0.761 bg_ri:0.776 p_pi:0.774 p_ri:0.844 r_pi:0.808 r_ri:0.859 t_pi:0.869 t_ri:0.828
 59%|█████▉    | 147/250 [7:59:28<5:28:46, 191.52s/it]#epoch:147 stage:1 train_loss:7.858e-03 val_loss:8.373e-03  time:3m11s

 bg_pi:0.758 bg_ri:0.779 p_pi:0.768 p_ri:0.844 r_pi:0.816 r_ri:0.858 t_pi:0.869 t_ri:0.828
 59%|█████▉    | 148/250 [8:02:40<5:25:47, 191.64s/it]#epoch:148 stage:1 train_loss:7.860e-03 val_loss:8.373e-03  time:3m12s

 bg_pi:0.760 bg_ri:0.779 p_pi:0.786 p_ri:0.837 r_pi:0.815 r_ri:0.844 t_pi:0.863 t_ri:0.835
 60%|█████▉    | 149/250 [8:05:50<5:21:37, 191.07s/it]#epoch:149 stage:1 train_loss:7.860e-03 val_loss:8.384e-03  time:3m10s

 bg_pi:0.760 bg_ri:0.780 p_pi:0.769 p_ri:0.837 r_pi:0.815 r_ri:0.846 t_pi:0.863 t_ri:0.829
 60%|██████    | 150/250 [8:09:02<5:18:39, 191.20s/it]#epoch:150 stage:1 train_loss:7.851e-03 val_loss:8.378e-03  time:3m11s

 bg_pi:0.760 bg_ri:0.764 p_pi:0.773 p_ri:0.833 r_pi:0.818 r_ri:0.850 t_pi:0.862 t_ri:0.835
 60%|██████    | 151/250 [8:12:14<5:16:00, 191.52s/it]#epoch:151 stage:1 train_loss:7.860e-03 val_loss:8.381e-03  time:3m12s

 bg_pi:0.755 bg_ri:0.781 p_pi:0.758 p_ri:0.836 r_pi:0.818 r_ri:0.853 t_pi:0.866 t_ri:0.826
 61%|██████    | 152/250 [8:15:19<5:09:48, 189.68s/it]#epoch:152 stage:1 train_loss:7.854e-03 val_loss:8.380e-03  time:3m5s

 bg_pi:0.758 bg_ri:0.784 p_pi:0.769 p_ri:0.842 r_pi:0.814 r_ri:0.856 t_pi:0.868 t_ri:0.827
 61%|██████    | 153/250 [8:18:31<5:07:49, 190.41s/it]#epoch:153 stage:1 train_loss:7.847e-03 val_loss:8.375e-03  time:3m12s

 bg_pi:0.757 bg_ri:0.772 p_pi:0.780 p_ri:0.835 r_pi:0.816 r_ri:0.845 t_pi:0.862 t_ri:0.834
 62%|██████▏   | 154/250 [8:21:37<5:02:26, 189.03s/it]#epoch:154 stage:1 train_loss:7.847e-03 val_loss:8.371e-03  time:3m6s

 bg_pi:0.758 bg_ri:0.775 p_pi:0.778 p_ri:0.836 r_pi:0.821 r_ri:0.844 t_pi:0.862 t_ri:0.834
 62%|██████▏   | 155/250 [8:24:48<5:00:19, 189.68s/it]#epoch:155 stage:1 train_loss:7.847e-03 val_loss:8.385e-03  time:3m11s

 bg_pi:0.755 bg_ri:0.789 p_pi:0.772 p_ri:0.841 r_pi:0.816 r_ri:0.856 t_pi:0.869 t_ri:0.827
 62%|██████▏   | 156/250 [8:27:54<4:55:17, 188.48s/it]#epoch:156 stage:1 train_loss:7.845e-03 val_loss:8.368e-03  time:3m6s

 bg_pi:0.758 bg_ri:0.777 p_pi:0.773 p_ri:0.838 r_pi:0.819 r_ri:0.848 t_pi:0.864 t_ri:0.832
 62%|██████▏   | 156/250 [8:28:49<5:06:35, 195.70s/it]
Traceback (most recent call last):
  File "train.py", line 248, in <module>
    train(args)
  File "train.py", line 74, in train
    train_procedure(args,model,model_save_dir)
  File "train.py", line 107, in train_procedure
    val_loss,all_pi,all_ri= val_epoch(model, criterion, val_dataloader)
  File "train.py", line 174, in val_epoch
    y_pred = np.array([output_sliding_voting(i,9) for i in out_pred])
  File "train.py", line 174, in <listcomp>
    y_pred = np.array([output_sliding_voting(i,9) for i in out_pred])
  File "train.py", line 23, in output_sliding_voting
    output = pd.Series(output).rolling(window).apply(lambda x : mode(x)[0][0]).fillna(method='bfill')
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/window/rolling.py", line 1843, in apply
    return super().apply(
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/window/rolling.py", line 1321, in apply
    return self._apply(
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/window/rolling.py", line 590, in _apply
    return self._apply_blockwise(homogeneous_func, name)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/window/rolling.py", line 442, in _apply_blockwise
    return self._apply_series(homogeneous_func, name)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/window/rolling.py", line 431, in _apply_series
    result = homogeneous_func(values)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/window/rolling.py", line 582, in homogeneous_func
    result = calc(values)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/window/rolling.py", line 579, in calc
    return func(x, start, end, min_periods, *numba_args)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/window/rolling.py", line 1348, in apply_func
    return window_func(values, begin, end, min_periods)
  File "pandas/_libs/window/aggregations.pyx", line 1315, in pandas._libs.window.aggregations.roll_apply
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/indexing.py", line 967, in __getitem__
    return self._getitem_axis(maybe_callable, axis=axis)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/indexing.py", line 1497, in _getitem_axis
    return self._get_slice_axis(key, axis=axis)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/indexing.py", line 1533, in _get_slice_axis
    return self.obj._slice(slice_obj, axis=axis)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/series.py", line 941, in _slice
    return self._get_values(slobj)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/series.py", line 1045, in _get_values
    return self._constructor(new_mgr).__finalize__(self)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/series.py", line 340, in __init__
    NDFrame.__init__(self, data)
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/pandas/core/generic.py", line 239, in __init__
    def __init__(
  File "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 43528) is killed by signal: Terminated. 

cuda is True
device is cuda
freeze
training with pre_train
linearprobe,freeze encoder
encoder.cls_token
False
encoder.pos_embed
False
encoder.patch_embed.proj.weight
False
encoder.patch_embed.proj.bias
False
encoder.blocks.0.norm1.weight
False
encoder.blocks.0.norm1.bias
False
encoder.blocks.0.attn.qkv.weight
False
encoder.blocks.0.attn.qkv.bias
False
encoder.blocks.0.attn.proj.weight
False
encoder.blocks.0.attn.proj.bias
False
encoder.blocks.0.norm2.weight
False
encoder.blocks.0.norm2.bias
False
encoder.blocks.0.mlp.fc1.weight
False
encoder.blocks.0.mlp.fc1.bias
False
encoder.blocks.0.mlp.fc2.weight
False
encoder.blocks.0.mlp.fc2.bias
False
encoder.blocks.1.norm1.weight
False
encoder.blocks.1.norm1.bias
False
encoder.blocks.1.attn.qkv.weight
False
encoder.blocks.1.attn.qkv.bias
False
encoder.blocks.1.attn.proj.weight
False
encoder.blocks.1.attn.proj.bias
False
encoder.blocks.1.norm2.weight
False
encoder.blocks.1.norm2.bias
False
encoder.blocks.1.mlp.fc1.weight
False
encoder.blocks.1.mlp.fc1.bias
False
encoder.blocks.1.mlp.fc2.weight
False
encoder.blocks.1.mlp.fc2.bias
False
encoder.blocks.2.norm1.weight
False
encoder.blocks.2.norm1.bias
False
encoder.blocks.2.attn.qkv.weight
False
encoder.blocks.2.attn.qkv.bias
False
encoder.blocks.2.attn.proj.weight
False
encoder.blocks.2.attn.proj.bias
False
encoder.blocks.2.norm2.weight
False
encoder.blocks.2.norm2.bias
False
encoder.blocks.2.mlp.fc1.weight
False
encoder.blocks.2.mlp.fc1.bias
False
encoder.blocks.2.mlp.fc2.weight
False
encoder.blocks.2.mlp.fc2.bias
False
encoder.blocks.3.norm1.weight
False
encoder.blocks.3.norm1.bias
False
encoder.blocks.3.attn.qkv.weight
False
encoder.blocks.3.attn.qkv.bias
False
encoder.blocks.3.attn.proj.weight
False
encoder.blocks.3.attn.proj.bias
False
encoder.blocks.3.norm2.weight
False
encoder.blocks.3.norm2.bias
False
encoder.blocks.3.mlp.fc1.weight
False
encoder.blocks.3.mlp.fc1.bias
False
encoder.blocks.3.mlp.fc2.weight
False
encoder.blocks.3.mlp.fc2.bias
False
encoder.blocks.4.norm1.weight
False
encoder.blocks.4.norm1.bias
False
encoder.blocks.4.attn.qkv.weight
False
encoder.blocks.4.attn.qkv.bias
False
encoder.blocks.4.attn.proj.weight
False
encoder.blocks.4.attn.proj.bias
False
encoder.blocks.4.norm2.weight
False
encoder.blocks.4.norm2.bias
False
encoder.blocks.4.mlp.fc1.weight
False
encoder.blocks.4.mlp.fc1.bias
False
encoder.blocks.4.mlp.fc2.weight
False
encoder.blocks.4.mlp.fc2.bias
False
encoder.blocks.5.norm1.weight
False
encoder.blocks.5.norm1.bias
False
encoder.blocks.5.attn.qkv.weight
False
encoder.blocks.5.attn.qkv.bias
False
encoder.blocks.5.attn.proj.weight
False
encoder.blocks.5.attn.proj.bias
False
encoder.blocks.5.norm2.weight
False
encoder.blocks.5.norm2.bias
False
encoder.blocks.5.mlp.fc1.weight
False
encoder.blocks.5.mlp.fc1.bias
False
encoder.blocks.5.mlp.fc2.weight
False
encoder.blocks.5.mlp.fc2.bias
False
encoder.blocks.6.norm1.weight
False
encoder.blocks.6.norm1.bias
False
encoder.blocks.6.attn.qkv.weight
False
encoder.blocks.6.attn.qkv.bias
False
encoder.blocks.6.attn.proj.weight
False
encoder.blocks.6.attn.proj.bias
False
encoder.blocks.6.norm2.weight
False
encoder.blocks.6.norm2.bias
False
encoder.blocks.6.mlp.fc1.weight
False
encoder.blocks.6.mlp.fc1.bias
False
encoder.blocks.6.mlp.fc2.weight
False
encoder.blocks.6.mlp.fc2.bias
False
encoder.blocks.7.norm1.weight
False
encoder.blocks.7.norm1.bias
False
encoder.blocks.7.attn.qkv.weight
False
encoder.blocks.7.attn.qkv.bias
False
encoder.blocks.7.attn.proj.weight
False
encoder.blocks.7.attn.proj.bias
False
encoder.blocks.7.norm2.weight
False
encoder.blocks.7.norm2.bias
False
encoder.blocks.7.mlp.fc1.weight
False
encoder.blocks.7.mlp.fc1.bias
False
encoder.blocks.7.mlp.fc2.weight
False
encoder.blocks.7.mlp.fc2.bias
False
encoder.blocks.8.norm1.weight
False
encoder.blocks.8.norm1.bias
False
encoder.blocks.8.attn.qkv.weight
False
encoder.blocks.8.attn.qkv.bias
False
encoder.blocks.8.attn.proj.weight
False
encoder.blocks.8.attn.proj.bias
False
encoder.blocks.8.norm2.weight
False
encoder.blocks.8.norm2.bias
False
encoder.blocks.8.mlp.fc1.weight
False
encoder.blocks.8.mlp.fc1.bias
False
encoder.blocks.8.mlp.fc2.weight
False
encoder.blocks.8.mlp.fc2.bias
False
encoder.blocks.9.norm1.weight
False
encoder.blocks.9.norm1.bias
False
encoder.blocks.9.attn.qkv.weight
False
encoder.blocks.9.attn.qkv.bias
False
encoder.blocks.9.attn.proj.weight
False
encoder.blocks.9.attn.proj.bias
False
encoder.blocks.9.norm2.weight
False
encoder.blocks.9.norm2.bias
False
encoder.blocks.9.mlp.fc1.weight
False
encoder.blocks.9.mlp.fc1.bias
False
encoder.blocks.9.mlp.fc2.weight
False
encoder.blocks.9.mlp.fc2.bias
False
encoder.blocks.10.norm1.weight
False
encoder.blocks.10.norm1.bias
False
encoder.blocks.10.attn.qkv.weight
False
encoder.blocks.10.attn.qkv.bias
False
encoder.blocks.10.attn.proj.weight
False
encoder.blocks.10.attn.proj.bias
False
encoder.blocks.10.norm2.weight
False
encoder.blocks.10.norm2.bias
False
encoder.blocks.10.mlp.fc1.weight
False
encoder.blocks.10.mlp.fc1.bias
False
encoder.blocks.10.mlp.fc2.weight
False
encoder.blocks.10.mlp.fc2.bias
False
encoder.blocks.11.norm1.weight
False
encoder.blocks.11.norm1.bias
False
encoder.blocks.11.attn.qkv.weight
False
encoder.blocks.11.attn.qkv.bias
False
encoder.blocks.11.attn.proj.weight
False
encoder.blocks.11.attn.proj.bias
False
encoder.blocks.11.norm2.weight
False
encoder.blocks.11.norm2.bias
False
encoder.blocks.11.mlp.fc1.weight
False
encoder.blocks.11.mlp.fc1.bias
False
encoder.blocks.11.mlp.fc2.weight
False
encoder.blocks.11.mlp.fc2.bias
False
encoder.norm.weight
False
encoder.norm.bias
False
upsample_1_1.weight
True
upsample_1_1.bias
True
upsample_2_1.weight
True
upsample_2_1.bias
True
upsample_3_1.weight
True
upsample_3_1.bias
True
upsample_4_1.weight
True
upsample_4_1.bias
True
upsample_1_2.weight
True
upsample_1_2.bias
True
upsample_2_2.weight
True
upsample_2_2.bias
True
upsample_3_2.weight
True
upsample_3_2.bias
True
upsample_4_2.weight
True
upsample_4_2.bias
True
conv_out.seq.0.weight
True
conv_out.seq.1.weight
True
conv_out.seq.1.bias
True
conv_cat1.seq.0.weight
True
conv_cat1.seq.1.weight
True
conv_cat1.seq.1.bias
True
conv_cat2.seq.0.weight
True
conv_cat2.seq.1.weight
True
conv_cat2.seq.1.bias
True
conv_cat3.seq.0.weight
True
conv_cat3.seq.1.weight
True
conv_cat3.seq.1.bias
True
conv_cat4.seq.0.weight
True
conv_cat4.seq.1.weight
True
conv_cat4.seq.1.bias
True
ECG_mae_segmentation_U_24(
  (encoder): EncoderMAE(
    (patch_embed): PatchEmbed_1D(
      (proj): Conv1d(1, 80, kernel_size=(24,), stride=(24,))
      (norm): Identity()
    )
    (blocks): ModuleList(
      (0): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (1): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (2): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (3): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (4): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (5): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (6): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (7): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (8): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (9): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (10): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
      (11): Block(
        (norm1): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (attn): Attention(
          (qkv): Linear(in_features=80, out_features=240, bias=True)
          (attn_drop): Dropout(p=0.0, inplace=False)
          (proj): Linear(in_features=80, out_features=80, bias=True)
          (proj_drop): Dropout(p=0.0, inplace=False)
        )
        (drop_path): Identity()
        (norm2): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
        (mlp): Mlp(
          (fc1): Linear(in_features=80, out_features=160, bias=True)
          (act): GELU()
          (drop1): Dropout(p=0.0, inplace=False)
          (fc2): Linear(in_features=160, out_features=80, bias=True)
          (drop2): Dropout(p=0.0, inplace=False)
        )
      )
    )
    (norm): LayerNorm((80,), eps=1e-05, elementwise_affine=True)
  )
  (upsample_1_1): ConvTranspose1d(80, 40, kernel_size=(9,), stride=(3,), padding=(3,))
  (upsample_2_1): ConvTranspose1d(40, 20, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_3_1): ConvTranspose1d(20, 10, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_4_1): ConvTranspose1d(10, 4, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_1_2): ConvTranspose1d(80, 40, kernel_size=(9,), stride=(3,), padding=(3,))
  (upsample_2_2): ConvTranspose1d(40, 20, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_3_2): ConvTranspose1d(20, 10, kernel_size=(8,), stride=(2,), padding=(3,))
  (upsample_4_2): ConvTranspose1d(10, 4, kernel_size=(8,), stride=(2,), padding=(3,))
  (conv_out): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(4, 4, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat1): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(80, 40, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat2): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(40, 20, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat3): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(20, 10, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
  (conv_cat4): CBR_1D(
    (seq): Sequential(
      (0): Conv1d(8, 4, kernel_size=(9,), stride=(1,), padding=(4,), bias=False)
      (1): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU()
    )
  )
)  0%|          | 0/1000 [00:00<?, ?it/s]
train_datasize 800 val_datasize 200
  0%|          | 1/1000 [01:06<18:25:11, 66.38s/it]#epoch:01 stage:1 train_loss:1.108e-02 val_loss:1.127e-02  time:1m6s

 bg_pi:0.485 bg_ri:0.950 p_pi:0.471 p_ri:0.872 r_pi:0.391 r_ri:0.713 t_pi:0.975 t_ri:0.068
